{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TASK 1: Batch output to filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment these lines if running from VSCode\n",
    "#import findspark\n",
    "#findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.streaming import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import window, col"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Build spark batch session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .appName(\"batch_app\")\\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/jpatokal/openflights/master/data/routes.dat\"\n",
    "\n",
    "sc.addFile(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Filter raw data to drop \\N values (null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+----+---+----+---+---+---+\n",
      "|_c0| _c1|_c2| _c3|_c4| _c5|_c6|_c7|_c8|\n",
      "+---+----+---+----+---+----+---+---+---+\n",
      "| 2P| 897|GES|2402|MNL|2397|  Y|  0|320|\n",
      "| 2P| 897|MNL|2397|GES|2402|  Y|  0|320|\n",
      "| 4M|3201|DFW|3670|EZE|3988|  Y|  0|777|\n",
      "| 4M|3201|EZE|3988|DFW|3670|  Y|  0|777|\n",
      "| 4M|3201|EZE|3988|JFK|3797|  Y|  0|777|\n",
      "| 4M|3201|JFK|3797|EZE|3988|  Y|  0|777|\n",
      "| 5N| 503|ARH|4362|CSH|6110|  Y|  0|AN4|\n",
      "| 5N| 503|ARH|4362|MMK|2949|  Y|  0|AN4|\n",
      "| 5N| 503|ARH|4362|USK|4369|  Y|  0|AN4|\n",
      "| 5N| 503|CSH|6110|ARH|4362|  Y|  0|AN4|\n",
      "+---+----+---+----+---+----+---+---+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df = spark.read.option(\"sep\", \",\").csv(\"file:///\" + SparkFiles.get(\"routes.dat\"))\n",
    "\n",
    "\n",
    "#Delete rows with \\N (null) values\n",
    "\n",
    "routes_filter_df = raw_df \\\n",
    "                   .filter(raw_df._c0 != \"\\\\N\") \\\n",
    "                   .filter(raw_df._c1 != \"\\\\N\") \\\n",
    "                   .filter(raw_df._c2 != \"\\\\N\") \\\n",
    "                   .filter(raw_df._c3 != \"\\\\N\") \\\n",
    "                   .filter(raw_df._c4 != \"\\\\N\") \\\n",
    "                   .filter(raw_df._c5 != \"\\\\N\") \\\n",
    "                   .filter(raw_df._c6 != \"\\\\N\") \\\n",
    "                   .filter(raw_df._c7 != \"\\\\N\") \\\n",
    "                   .filter(raw_df._c8 != \"\\\\N\") \\\n",
    "\n",
    "routes_filter_df.show(10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Renaming Columns for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------+-----------------+-------------------+----------------------+---------+-----+---------+\n",
      "|Airline|Airline_ID|Source_airport|Source_airport_ID|Destination_airport|Destination_airport_ID|Codeshare|Stops|Equipment|\n",
      "+-------+----------+--------------+-----------------+-------------------+----------------------+---------+-----+---------+\n",
      "|     2P|       897|           GES|             2402|                MNL|                  2397|        Y|    0|      320|\n",
      "|     2P|       897|           MNL|             2397|                GES|                  2402|        Y|    0|      320|\n",
      "|     4M|      3201|           DFW|             3670|                EZE|                  3988|        Y|    0|      777|\n",
      "|     4M|      3201|           EZE|             3988|                DFW|                  3670|        Y|    0|      777|\n",
      "|     4M|      3201|           EZE|             3988|                JFK|                  3797|        Y|    0|      777|\n",
      "|     4M|      3201|           JFK|             3797|                EZE|                  3988|        Y|    0|      777|\n",
      "|     5N|       503|           ARH|             4362|                CSH|                  6110|        Y|    0|      AN4|\n",
      "|     5N|       503|           ARH|             4362|                MMK|                  2949|        Y|    0|      AN4|\n",
      "|     5N|       503|           ARH|             4362|                USK|                  4369|        Y|    0|      AN4|\n",
      "|     5N|       503|           CSH|             6110|                ARH|                  4362|        Y|    0|      AN4|\n",
      "|     5N|       503|           MMK|             2949|                ARH|                  4362|        Y|    0|      AN4|\n",
      "|     5N|       503|           MMK|             2949|                TOS|                   663|        Y|    0|      AN4|\n",
      "|     5N|       503|           TOS|              663|                MMK|                  2949|        Y|    0|      AN4|\n",
      "|     5N|       503|           USK|             4369|                ARH|                  4362|        Y|    0|      AN4|\n",
      "|     5T|      1623|           YBK|               29|                YCS|                  5487|        Y|    0|      ATR|\n",
      "|     5T|      1623|           YBK|               29|                YXN|                  5534|        Y|    0|      ATR|\n",
      "|     5T|      1623|           YCS|             5487|                YBK|                    29|        Y|    0|      AT4|\n",
      "|     5T|      1623|           YCS|             5487|                YRT|                   132|        Y|    0|  ATR AT4|\n",
      "|     5T|      1623|           YEK|               50|                YXN|                  5534|        Y|    0|  ATR AT4|\n",
      "|     5T|      1623|           YEK|               50|                YYQ|                   187|        Y|    0|  ATR AT4|\n",
      "+-------+----------+--------------+-----------------+-------------------+----------------------+---------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new=routes_filter_df.withColumnRenamed(\"_c0\",\"Airline\")\\\n",
    "    .withColumnRenamed(\"_c1\",\"Airline_ID\")\\\n",
    "    .withColumnRenamed(\"_c2\",\"Source_airport\")\\\n",
    "    .withColumnRenamed(\"_c3\",\"Source_airport_ID\")\\\n",
    "    .withColumnRenamed(\"_c4\",\"Destination_airport\")\\\n",
    "    .withColumnRenamed(\"_c5\",\"Destination_airport_ID\")\\\n",
    "    .withColumnRenamed(\"_c6\",\"Codeshare\")\\\n",
    "    .withColumnRenamed(\"_c7\",\"Stops\")\\\n",
    "    .withColumnRenamed(\"_c8\",\"Equipment\")\n",
    "   \n",
    "df_new.show()\n",
    "\n",
    "df_new.createOrReplaceTempView(\"flights_information\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Select top 10 airports by source airport using spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Source_airport|Count|\n",
      "+--------------+-----+\n",
      "|           ATL|  633|\n",
      "|           ORD|  277|\n",
      "|           LAX|  209|\n",
      "|           LHR|  202|\n",
      "|           VIE|  159|\n",
      "|           CDG|  155|\n",
      "|           FRA|  150|\n",
      "|           AMS|  145|\n",
      "|           DFW|  143|\n",
      "|           DEN|  136|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#In spark.sql\n",
    "\n",
    "top_airports = spark.sql(\"\"\"\n",
    "SELECT Source_airport, COUNT(Source_airport) as Count \n",
    "FROM flights_information \n",
    "GROUP BY Source_Airport\n",
    "ORDER BY Count DESC LIMIT 10\n",
    " \"\"\")\n",
    "\n",
    "top_airports.show()\n",
    "\n",
    "#dense_rank can be used"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Select top 10 airports by source airport using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Source_airport|count|\n",
      "+--------------+-----+\n",
      "|           ATL|  633|\n",
      "|           ORD|  277|\n",
      "|           LAX|  209|\n",
      "|           LHR|  202|\n",
      "|           VIE|  159|\n",
      "|           CDG|  155|\n",
      "|           FRA|  150|\n",
      "|           AMS|  145|\n",
      "|           DFW|  143|\n",
      "|           DEN|  136|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_airports = df_new.select(\"Source_airport\") \\\n",
    "                        .groupBy(\"Source_airport\").count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .limit(10)\n",
    "\n",
    "top_airports.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Print result to filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output .csv file saved in ./output/batch\n"
     ]
    }
   ],
   "source": [
    "#Write to sink\n",
    "\n",
    "csvPath = \"./output/batch\"\n",
    "top_airports.write.mode(\"overwrite\").option(\"header\", \"true\") \\\n",
    ".csv(csvPath)\n",
    "\n",
    "print(\"Output .csv file saved in\", csvPath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Stop batch session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TASK 2: Convert to Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkDirectory = \"./output/streaming/check\"\n",
    "\n",
    "  #Define schema required for Structured streaming\n",
    "\n",
    "fileSchema = (StructType()\n",
    "  .add(StructField(\"Airline\", StringType(), True))\n",
    "  .add(StructField(\"Airline_ID\", IntegerType()))\n",
    "  .add(StructField(\"Source_airport\", StringType(), True))\n",
    "  .add(StructField(\"Source_airport_ID\", IntegerType()))\n",
    "  .add(StructField(\"Destination_airport\", StringType(), True))\n",
    "  .add(StructField(\"Destination_airport_ID\", IntegerType()))\n",
    "  .add(StructField(\"Codeshare\", StringType(), True))\n",
    "  .add(StructField(\"Stops\", IntegerType()))\n",
    "  .add(StructField(\"Equipment\", StringType(), True))\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF_streaming = (spark\n",
    "  .readStream\n",
    "  .schema(fileSchema)\n",
    "  .option(\"header\", False)\n",
    "  .option(\"maxFilesperTrigger \",1)\n",
    "  .csv(SparkFiles.getRootDirectory())) #stream needs to read from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputDF_streaming.writeStream \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkpointLocation\", checkDirectory) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " #cleaning & including dummy timestamp\n",
    "    \n",
    "from pyspark.sql import Row\n",
    "    \n",
    "transformed_df = inputDF_streaming \\\n",
    "                   .filter(inputDF_streaming.Airline.isNotNull()) \\\n",
    "                   .filter(inputDF_streaming.Airline_ID.isNotNull()) \\\n",
    "                   .filter(inputDF_streaming.Source_airport.isNotNull()) \\\n",
    "                   .filter(inputDF_streaming.Source_airport_ID.isNotNull()) \\\n",
    "                   .filter(inputDF_streaming.Destination_airport.isNotNull()) \\\n",
    "                   .filter(inputDF_streaming.Destination_airport_ID.isNotNull()) \\\n",
    "                   .filter(inputDF_streaming.Codeshare.isNotNull()) \\\n",
    "                   .filter(inputDF_streaming.Stops.isNotNull()) \\\n",
    "                   .filter(inputDF_streaming.Equipment.isNotNull())\n",
    "    \n",
    "    \n",
    "transformed_df_withEvents = transformed_df \\\n",
    "     .selectExpr(\n",
    "    \"*\",\n",
    "    \"cast(cast(Destination_airport_ID as int)/1000000 as timestamp) as dummy_event_time\"\n",
    "  )\n",
    "\n",
    "transformed_df_withEvents = transformed_df_withEvents \\\n",
    "                        .filter((transformed_df_withEvents.dummy_event_time.isNotNull()))\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Select top airports (streaming_top_airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Source_airport: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n",
      "Streaming DataFrame : True\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "# Select the source airport column and count the occurrences\n",
    "\n",
    "#transformed_df.createOrReplaceTempView(\"flights_information\")\n",
    "\n",
    "\n",
    "streaming_top_airports = transformed_df_withEvents.select(\"Source_airport\") \\\n",
    "                        .groupBy(\"Source_airport\").count() \\\n",
    "                        .orderBy(desc(\"count\")) \\\n",
    "                        .limit(10)\n",
    "\n",
    "\n",
    "checkDirectory = \"./output/streaming/check\"\n",
    "\n",
    "streaming_top_airports.printSchema()\n",
    "\n",
    "\n",
    "print(\"Streaming DataFrame : \" + str(streaming_top_airports.isStreaming))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the streaming output to the console\n",
    "\n",
    "checkDirectory = \"./output/streaming/check\"\n",
    "\n",
    "streamingQuery = streaming_top_airports \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .option(\"checkDir\", checkDirectory ) \\\n",
    "    .trigger(processingTime=\"10 minutes\") \\\n",
    "    .start()\n",
    "\n",
    "#.awaitTermination() \n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK 3: Aggregations using sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- Source_airport: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a window with a time interval of 1 minute\n",
    "\n",
    "from pyspark.sql.functions import window, col\n",
    "\n",
    "streaming_df = transformed_df_withEvents \\\n",
    "    .groupBy(window(col(\"dummy_event_time\"), \"10 minutes\", \"5 minutes\"), col(\"Source_airport\"))\\\n",
    "    .count() \\\n",
    "    .orderBy(desc(\"count\")) \\\n",
    "    .limit(10)\n",
    "\n",
    "streaming_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write output to console\n",
    "\n",
    "streamingQuery = streaming_df \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(processingTime=\"20 seconds\") \\\n",
    "    .start() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
