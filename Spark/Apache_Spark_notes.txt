************ LEARNING SPARK ****************

***********************************************************
1. INTRODUCTION TO APACHE SPARK: A UNIFIED ANALYTICS ENGINE
***********************************************************

- Google -> Google File System (GFS), MapReduce (MR) and Bigtable
- Yahoo -> Hadoop File System (HDFS)
- AMPLab UC Berkeley -> Spark

Apache Spark is a unified engine designed for large-scale distributed data processing,
on premises in data centers or in the cloud.

Spark provides in-memory storage for intermediate computations, making it much
faster than Hadoop MapReduce. It incorporates libraries with composable APIs for
machine learning (MLlib), SQL for interactive queries (Spark SQL), stream processing
(Structured Streaming) for interacting with real-time data, and graph processing
(GraphX).

Spark’s design philosophy centers around four key characteristics:

• Speed
• Ease of use
• Modularity
• Extensibility

Apache Spark Components as a Unified Stack:

• SparkSQL and Dataframes + Datasets
lw7rmdRdcdUv
// In Scala
// Read data off Amazon S3 bucket into a Spark DataFrame
spark.read.json("s3://apache_spark/data/committers.json")
.createOrReplaceTempView("committers")
// Issue a SQL query and return the result as a Spark DataFrame
val results = spark.sql("""SELECT name, org, module, release, num_commits
FROM committers WHERE module = 'mllib' AND num_commits > 10
ORDER BY num_commits DESC""")


• Spark Structured Streaming

# In Python
# Read a stream from a local host
from pyspark.sql.functions import explode, split
lines = (spark
.readStream
.format("socket")
.option("host", "localhost")
.option("port", 9999)
.load())
# Perform transformation
# Split the lines into words
words = lines.select(explode(split(lines.value, " ")).alias("word"))
# Generate running word count
word_counts = words.groupBy("word").count()
# Write out to the stream to Kafka
query = (word_counts
.writeStream
.format("kafka")
.option("topic", "output"))

• Machine Learning MLib

# In Python
from pyspark.ml.classification import LogisticRegression
...
training = spark.read.csv("s3://...")
test = spark.read.csv("s3://...")
# Load training data
lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)
# Fit the model
lrModel = lr.fit(training)
# Predict
lrModel.transform(test)

• Graph Processing Graph X

// In Scala
val graph = Graph(vertices, edges)
messages = spark.textFile("hdfs://...")
val graph2 = graph.joinVertices(messages) {
(id, vertex, msg) => ...
}

- Spark Core and Spark SQL Engine -> Scala, SQL, Python, Java, R


**Apache Spark’s Distributed Execution**

- Spark Driver -> communicate with the cluster manager, request resources for executors...

- Spark Session -> unified conduit to all Spark operations

// In Scala
import org.apache.spark.sql.SparkSession
// Build SparkSession
val spark = SparkSession
.builder
.appName("LearnSpark")
.config("spark.sql.shuffle.partitions", 6)
.getOrCreate()
...
// Use the session to read JSON
val people = spark.read.json("...")
...
// Use the session to issue a SQL query
val resultsDF = spark.sql("SELECT city, pop, state, zip FROM table_name")

- Cluster manager -> manages and allocates resources for the cluster of nodes

- Spark executors -> run on each worker node in the cluster, communicate with the driver program and execute tasks on workers

- Deployment modes -> Spark is supported for myriad deployment modes

- Distributed data and partitions -> physical data is distributed accross storage as partitions in HDFS or cloud
									partition allows for efficient parallelism
									
									# In Python
									log_df = spark.read.text("path_to_large_text_file").repartition(8)
									print(log_df.rdd.getNumPartitions())
									

***********************************************
2. DOWNLOADING APACHE SPARK AND GETTING STARTED
***********************************************

- Download Java, Python, winutils (for Hadoop on windows)
- Add system variables to path

Spark Application Concepts:

- Application -> A user program built on Spark using its APIs
- SparkSession -> A user program built on Spark using its APIs. It consists of a driver program and executors on the cluster.
- Job -> parallel computation consisting of multiple tasks that gets spawned in response to a Spark action
- Stage -> Each job gets divided into smaller sets of tasks called stages that depend on each other.
- Task -> single unit of work or execution that will be sent to a Spark executor.

Transformations, Actions, and Lazy Evaluation

- Transformations -> transform a Spark DataFrame into a new DataFrame without altering the original data (orderBy, groupBy, select, filter, join)
- Actions -> show, take, count, collect, save
- Lazy evaluation -> results are not computed immediately, execution delayed until an action is invoked or data is touched

# In Python
>>> strings = spark.read.text("../README.md")
>>> filtered = strings.filter(strings.value.contains("Spark"))
>>> filtered.count() -> action triggers the execution

// In Scala
scala> import org.apache.spark.sql.functions._
scala> val strings = spark.read.text("../README.md")
scala> val filtered = strings.filter(col("value").contains("Spark"))
scala> filtered.count()

Transformations can be classified as having either narrow dependencies or wide dependencies. Any transformation where a single 
output partition can be computed from a single input partition is a narrow transformation.

However, groupBy() or orderBy() instruct Spark to perform wide transformations, where data from other partitions is 
read in, combined, and written to disk.

The Spark UI -> http://localhost:4040

Building Standalone Applications in Scala -> Scala Build Tool (sbt)

*********************************
3.APACHE SPARK'S STRUCTURED APIs
*********************************

Resilient Distributed Dataset (RDD) 

• Dependencies -> instructs Spark how an RDD is constructed with its inputs 
• Partitions (with some locality information) -> provide Spark the ability to split the work to parallelize computation on partitions across executors
• Compute function: Partition => Iterator[T] -> produces an Iterator[T] for the data that will be stored in the RDD.

Structuring Spark

Spark 2.x introduced a few key schemes for structuring Spark. One is to express computations by using common patterns found in data analysis.

- low-level RDD API:

# In Python
# Create an RDD of tuples (name, age)
dataRDD = sc.parallelize([("Brooke", 20), ("Denny", 31), ("Jules", 30),
("TD", 35), ("Brooke", 25)])
# Use map and reduceByKey transformations with their lambda
# expressions to aggregate and then compute average
agesRDD = (dataRDD
.map(lambda x: (x[0], (x[1], 1)))
.reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
.map(lambda x: (x[0], x[1][0]/x[1][1])))

- high-level DSL operators and the DataFrame API:

# In Python
from pyspark.sql import SparkSession
from pyspark.sql.functions import avg
# Create a DataFrame using SparkSession
spark = (SparkSession
.builder
.appName("AuthorsAges")
.getOrCreate())
# Create a DataFrame
data_df = spark.createDataFrame([("Brooke", 20), ("Denny", 31), ("Jules", 30),
("TD", 35), ("Brooke", 25)], ["name", "age"])
# Group the same names together, aggregate their ages, and compute an average
avg_df = data_df.groupBy("name").agg(avg("age"))
# Show the results of the final execution
avg_df.show()

+------+--------+
| name|avg(age)|
+------+--------+
|Brooke| 22.5|
| Jules| 30.0|
| TD| 35.0|
| Denny| 31.0|
+------+--------+

// In Scala
import org.apache.spark.sql.functions.avg
import org.apache.spark.sql.SparkSession
// Create a DataFrame using SparkSession
val spark = SparkSession
.builder
.appName("AuthorsAges")
.getOrCreate()
// Create a DataFrame of names and ages
val dataDF = spark.createDataFrame(Seq(("Brooke", 20), ("Brooke", 25),
("Denny", 31), ("Jules", 30), ("TD", 35))).toDF("name", "age")
// Group the same names together, aggregate their ages, and compute an average
val avgDF = dataDF.groupBy("name").agg(avg("age"))
// Show the results of the final execution
avgDF.show()

+------+--------+
| name|avg(age)|
+------+--------+
|Brooke| 22.5|
| Jules| 30.0|
| TD| 35.0|
| Denny| 31.0|
+------+--------+

- The DataFrame API

Spark DataFrames are like distributed in-memory tables with named columns and
schemas, where each column has a specific data type: integer, string, array, map, real,
date, timestamp, etc.

Spark’s Basic Data Types

Basic Scala data types in Spark

Data type | Value assigned in Scala | API to instantiate

ByteType Byte DataTypes.ByteType
ShortType Short DataTypes.ShortType
IntegerType Int DataTypes.IntegerType
LongType Long DataTypes.LongType
FloatType Float DataTypes.FloatType
DoubleType Double DataTypes.DoubleType
StringType String DataTypes.StringType
BooleanType Boolean DataTypes.BooleanType
DecimalType java.math.BigDecimal DecimalType

Basic Python data types in Spark

Data type | Value assigned in Python | API to instantiate

ByteType int DataTypes.ByteType
ShortType int DataTypes.ShortType
IntegerType int DataTypes.IntegerType
LongType int DataTypes.LongType
FloatType float DataTypes.FloatType
DoubleType float DataTypes.DoubleType
StringType str DataTypes.StringType
BooleanType bool DataTypes.BooleanType
DecimalType decimal.Decimal DecimalType


Spark’s Structured and Complex Data Types

Scala structured data types in Spark

Data type | Value assigned in Scala | API to instantiate

BinaryType Array[Byte] DataTypes.BinaryType
Timestamp Type java.sql.Timestamp DataTypes.TimestampType
DateType java.sql.Date DataTypes.DateType
ArrayType scala.collection.Seq DataTypes.createArrayType(Element Type)
MapType scala.collection.Map DataTypes.createMapType(keyType, valueType)
StructType org.apache.spark.sql.Row StructType(ArrayType[fieldTypes])
StructField A value type corresponding to the type of this field StructField(name, dataType, [nullable])


Python structured data types in Spark

Data type | Value assigned in Python | API to instantiate

BinaryType bytearray BinaryType()
TimestampType datetime.datetime TimestampType()
DateType datetime.date DateType()
ArrayType List, tuple, or array ArrayType(dataType, [nullable])
MapType dict MapType(keyType, valueType, [nullable])
StructType List or tuple StructType([fields])
StructField A value type corresponding to the type of this field StructField(name, dataType, [nullable])

Schemas and Creating DataFrames

A schema in Spark defines the column names and associated data types for a DataFrame.

Defining a schema up front as opposed to taking a schema-on-read approach offers three benefits:

• You relieve Spark from the onus of inferring data types.
• You prevent Spark from creating a separate job just to read a large portion of your file to ascertain the schema, 
  which for a large data file can be expensive and time-consuming.
• You can detect errors early if data doesn’t match the schema.

- Two ways to define a schema:

DataFrame API

// In Scala
import org.apache.spark.sql.types._
val schema = StructType(Array(StructField("author", StringType, false),
StructField("title", StringType, false),
StructField("pages", IntegerType, false)))

# In Python
from pyspark.sql.types import *
schema = StructType([StructField("author", StringType(), False),
StructField("title", StringType(), False),
StructField("pages", IntegerType(), False)])

using DDL

// In Scala
val schema = "author STRING, title STRING, pages INT"
# In Python
schema = "author STRING, title STRING, pages INT"


- Create a DataFrame using the schema defined above
blogs_df = spark.createDataFrame(data, schema)

$>blogs_df.schema

StructType(List(StructField("Id",IntegerType,false),
StructField("First",StringType,false),
StructField("Last",StringType,false),
...

- data from a JSON file:

// In Scala
val blogsDF = spark.read.schema(schema).json(jsonFile)

Columns and Expressions

// In Scala
scala> import org.apache.spark.sql.functions._
scala> blogsDF.columns
res2: Array[String] = Array(Campaigns, First, Hits, Id, Last, Published, Url)
// Access a particular column with col and it returns a Column type
scala> blogsDF.col("Id")
res3: org.apache.spark.sql.Column = id
// Use an expression to compute a value
scala> blogsDF.select(expr("Hits * 2")).show(2)
// or use col to compute value
scala> blogsDF.select(col("Hits") * 2).show(2)

Rows

// In Scala
import org.apache.spark.sql.Row
// Create a Row
val blogRow = Row(6, "Reynold", "Xin", "https://tinyurl.6", 255568, "3/2/2015",
Array("twitter", "LinkedIn"))
// Access using index for individual items
blogRow(1)
res62: Any = Reynold
# In Python
from pyspark.sql import Row
blog_row = Row(6, "Reynold", "Xin", "https://tinyurl.6", 255568, "3/2/2015",
["twitter", "LinkedIn"])
# access using index for individual items
blog_row[1]
'Reynold'

Row objects can be used to create DataFrames if you need them for quick interactivity
and exploration:

# In Python
rows = [Row("Matei Zaharia", "CA"), Row("Reynold Xin", "CA")]
authors_df = spark.createDataFrame(rows, ["Authors", "State"])
authors_df.show()

// In Scala
val rows = Seq(("Matei Zaharia", "CA"), ("Reynold Xin", "CA"))
val authorsDF = rows.toDF("Author", "State")
authorsDF.show()

Using DataFrameReader and DataFrameWriter

If you don’t want to specify the schema, Spark can infer schema from a sample at a lesser cost. For example, you can use the samplingRatio option:

// In Scala
val sampleDF = spark
.read
.option("samplingRatio", 0.001)
.option("header", true)
.csv("""/databricks-datasets/learning-spark-v2/
sf-fire/sf-fire-calls.csv""")

# In Python, define a schema

from pyspark.sql.types import *

# Programmatic way to define a schema

fire_schema = StructType([StructField('CallNumber', IntegerType(), True),
StructField('UnitID', StringType(), True),
StructField('IncidentNumber', IntegerType(), True),
StructField('CallType', StringType(), True)
....

# Use the DataFrameReader interface to read a CSV file
sf_fire_file = "/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv"
fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)

// In Scala it would be similar

val fireSchema = StructType(Array(StructField("CallNumber", IntegerType, true),
The DataFrame API | 59
StructField("UnitID", StringType, true)
...

// Read the file using the CSV DataFrameReader
val sfFireFile="/databricks-datasets/learning-spark-v2/sf-fire/sf-fire-calls.csv"
val fireDF = spark.read.schema(fireSchema)
.option("header", "true")
.csv(sfFireFile)

The spark.read.csv() function reads in the CSV file and returns a DataFrame of
rows and named columns with the types dictated in the schema.

To write the DataFrame into an external data source in your format of choice, you
can use the DataFrameWriter interface.

- Saving a DataFrame as a Parquet file or SQL table:

// In Scala to save as a Parquet file
val parquetPath = ...
fireDF.write.format("parquet").save(parquetPath)
# In Python to save as a Parquet file
parquet_path = ...
fire_df.write.format("parquet").save(parquet_path)

// In Scala to save as a table
val parquetTable = ... // name of the table
fireDF.write.format("parquet").saveAsTable(parquetTable)
# In Python
parquet_table = ... # name of the table
fire_df.write.format("parquet").saveAsTable(parquet_table)

- Transformations and actions:

Projections and filters. A projection in relational parlance is a way to return only the
rows matching a certain relational condition by using filters.

In Spark, projections are
done with the select() method, while filters can be expressed using the filter() or
where() method.

# In Python
few_fire_df = (fire_df
.select("IncidentNumber", "AvailableDtTm", "CallType")
.where(col("CallType") != "Medical Incident"))
few_fire_df.show(5, truncate=False)
// In Scala
val fewFireDF = fireDF
.select("IncidentNumber", "AvailableDtTm", "CallType")
.where($"CallType" =!= "Medical Incident")
fewFireDF.show(5, false)


- Renaming, adding, and dropping columns

# In Python
new_fire_df = fire_df.withColumnRenamed("Delay", "ResponseDelayedinMins")
(new_fire_df
.select("ResponseDelayedinMins")
.where(col("ResponseDelayedinMins") > 5)
.show(5, False))
// In Scala
val newFireDF = fireDF.withColumnRenamed("Delay", "ResponseDelayedinMins")
newFireDF
.select("ResponseDelayedinMins")
.where($"ResponseDelayedinMins" > 5)
.show(5, false)

Because DataFrame transformations are immutable, when we
rename a column using withColumnRenamed() we get a new DataFrame 
while retaining the original with the old column name.

spark.sql.functions has a set of to/from date/timestamp
functions such as to_timestamp() and to_date() that we can use for just this
purpose:


# In Python
fire_ts_df = (new_fire_df
.withColumn("IncidentDate", to_timestamp(col("CallDate"), "MM/dd/yyyy"))
.drop("CallDate")
.withColumn("OnWatchDate", to_timestamp(col("WatchDate"), "MM/dd/yyyy"))
.drop("WatchDate")
.withColumn("AvailableDtTS", to_timestamp(col("AvailableDtTm"),
"MM/dd/yyyy hh:mm:ss a"))
.drop("AvailableDtTm"))
# Select the converted columns
(fire_ts_df
.select("IncidentDate", "OnWatchDate", "AvailableDtTS")
.show(5, False))


// In Scala
val fireTsDF = newFireDF
.withColumn("IncidentDate", to_timestamp(col("CallDate"), "MM/dd/yyyy"))
.drop("CallDate")
.withColumn("OnWatchDate", to_timestamp(col("WatchDate"), "MM/dd/yyyy"))
.drop("WatchDate")
.withColumn("AvailableDtTS", to_timestamp(col("AvailableDtTm"),
"MM/dd/yyyy hh:mm:ss a"))
.drop("AvailableDtTm")
// Select the converted columns
fireTsDF
.select("IncidentDate", "OnWatchDate", "AvailableDtTS")
.show(5, false)


1. Convert the existing column’s data type from string to a Spark-supported timestamp.
2. Use the new format specified in the format string "MM/dd/yyyy" or "MM/dd/yyyy hh:mm:ss a" where appropriate.
3. After converting to the new data type, drop() the old column and append the new one specified in the first argument to the withColumn() method.
4. Assign the new modified DataFrame to fire_ts_df.


- Aggregations:

A handful of transformations and actions on DataFrames, such as groupBy(),
orderBy(), and count(), offer the ability to aggregate by column names and then
aggregate counts across them.

# In Python
(fire_ts_df
.select("CallType")
.where(col("CallType").isNotNull())
.groupBy("CallType")
.count()
.orderBy("count", ascending=False)
.show(n=10, truncate=False))

// In Scala
fireTsDF
.select("CallType")
.where(col("CallType").isNotNull)
.groupBy("CallType")
.count()
.orderBy(desc("count"))
.show(10, false)

The DataFrame API also offers the collect() method, but for
extremely large DataFrames this is resource-heavy (expensive) and
dangerous, as it can cause out-of-memory (OOM) exceptions.
Unlike count(), which returns a single number to the driver, col
lect() returns a collection of all the Row objects in the entire Data‐
Frame or Dataset. If you want to take a peek at some Row records
you’re better off with take(n), which will return only the first n
Row objects of the DataFrame.


- Other common DataFrame operations

min(), max(), sum(), avg()

- The Dataset API

Spark 2.0 unified the DataFrame and Dataset APIs as Structured APIs with similar interfaces so that 
developers would only have to learn a single set of APIs. Datasets take on two characteristics: typed and untyped APIs

Typed Objects, Untyped Objects, and Generic Rows

In Spark’s supported languages, Datasets make sense only in Java and Scala, whereas
in Python and R only DataFrames make sense. This is because Python and R are not
compile-time type-safe; types are dynamically inferred or assigned during execution,
not during compile time. The reverse is true in Scala and Java: types are bound to
variables and objects at compile time. In Scala, however, a DataFrame is just an alias
for untyped Dataset[Row].

Using an index into the Row object, you can access individual fields with its public
getter methods:

// In Scala
row.getInt(0)
res23: Int = 350
row.getBoolean(1)
res24: Boolean = true
row.getString(2)
res25: String = Learning Spark 2E

# In Python
row[0]
Out[13]: 350
row[1]
Out[14]: True
row[2]
Out[15]: 'Learning Spark 2E'

- Creating Datasets

When creating a Dataset in Scala, the easiest way to specify the schema for the resulting Dataset is to use a case class

case class DeviceIoTData (battery_level: Long, c02_level: Long,
cca2: String, cca3: String, cn: String, device_id: Long,
device_name: String, humidity: Long, ip: String, latitude: Double,
lcd: String, longitude: Double, scale:String, temp: Long,
timestamp: Long)

Once defined, we can use it to read our file and convert the returned Dataset[Row]
into Dataset[DeviceIoTData]

// In Scala
val ds = spark.read
.json("/databricks-datasets/learning-spark-v2/iot-devices/iot_devices.json")
.as[DeviceIoTData]

- Dataset Operations

// In Scala
val filterTempDS = ds.filter({d => {d.temp > 30 && d.humidity > 70})
filterTempDS: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level...]
filterTempDS.show(5, false)

The operations we can perform on Datasets—filter(), map(), groupBy(),
select(), take(), etc.—are similar to the ones on DataFrames.

When we use Datasets, the underlying Spark SQL engine handles the creation, conversion,
serialization, and deserialization of the JVM objects.


Spark SQL and the Underlying Engine

At a programmatic level, Spark SQL allows developers to issue ANSI SQL:2003–compatible
queries on structured data with a schema.

At the core of the Spark SQL engine are the Catalyst optimizer and Project Tungsten.

The Catalyst Optimizer

The Catalyst optimizer takes a computational query and converts it into an execution
plan.

1. Analysis
2. Logical optimization
3. Physical planning
4. Code generation

Regardless of the language you use, your computation undergoes the same journey
and the resulting bytecode is likely the same:

# In Python
count_mnm_df = (mnm_df
.select("State", "Color", "Count")
.groupBy("State", "Color")
.agg(count("Count")
.alias("Total"))
.orderBy("Total", ascending=False))


-- In SQL
SELECT State, Color, Count, sum(Count) AS Total
FROM MNM_TABLE_NAME
GROUP BY State, Color, Count
ORDER BY Total DESC

To see the different stages the Python code goes through, you can use the
count_mnm_df.explain(True) method on the DataFrame. Or, to get a look at the different
logical and physical plans, in Scala you can call df.queryExecution.logical
or df.queryExecution.optimizedPlan.


******************************************************************
4.SPARK SQL AND DATAFRAMES: INTRODUCTION TO BUILT-IN DATA SOURCES
******************************************************************

In particular, Spark SQL:

• Provides the engine upon which the high-level Structured APIs we explored in
Chapter 3 are built.
• Can read and write data in a variety of structured formats (e.g., JSON, Hive
tables, Parquet, Avro, ORC, CSV).
• Lets you query data using JDBC/ODBC connectors from external business intelligence
(BI) data sources such as Tableau, Power BI, Talend, or from RDBMSs
such as MySQL and PostgreSQL.
• Provides a programmatic interface to interact with structured data stored as
tables or views in a database from a Spark application
• Offers an interactive shell to issue SQL queries on your structured data.
• Supports ANSI SQL:2003-compliant commands and HiveQL.


To issue any SQL query, use the sql() method on the SparkSession instance, spark,
such as spark.sql("SELECT * FROM myTableName"). All spark.sql queries executed
in this manner return a DataFrame on which you may perform further Spark operations

- Basic Query Examples:

// In Scala
import org.apache.spark.sql.SparkSession
val spark = SparkSession
.builder
.appName("SparkSQLExampleApp")
.getOrCreate()
// Path to data set
val csvFile="/databricks-datasets/learning-spark-v2/flights/departuredelays.csv"
// Read and create a temporary view
// Infer schema (note that for larger files you may want to specify the schema)
val df = spark.read.format("csv")
.option("inferSchema", "true")
.option("header", "true")
.load(csvFile)
// Create a temporary view
df.createOrReplaceTempView("us_delay_flights_tbl")

# In Python
from pyspark.sql import SparkSession
# Create a SparkSession
spark = (SparkSession
.builder
.appName("SparkSQLExampleApp")
.getOrCreate())
# Path to data set
csv_file = "/databricks-datasets/learning-spark-v2/flights/departuredelays.csv"
# Read and create a temporary view
# Infer schema (note that for larger files you
# may want to specify the schema)
df = (spark.read.format("csv")
.option("inferSchema", "true")
.option("header", "true")
.load(csv_file))
df.createOrReplaceTempView("us_delay_flights_tbl")

If you want to specify a schema, you can use a DDL-formatted
string. For example:

// In Scala
val schema = "date STRING, delay INT, distance INT,
origin STRING, destination STRING"

# In Python
schema = "`date` STRING, `delay` INT, `distance` INT,
`origin` STRING, `destination` STRING"

Now that we have a temporary view, we can issue SQL queries using Spark SQL.

spark.sql("""SELECT distance, origin, destination
FROM us_delay_flights_tbl WHERE distance > 1000
ORDER BY distance DESC""").show(10)


SQL Tables and Views

Instead of having a separate metastore for Spark tables, Spark by default uses the
Apache Hive metastore, located at /user/hive/warehouse, to persist all the metadata
about your tables.

Managed Versus UnmanagedTables

Spark allows you to create two types of tables: managed and unmanaged. For a managed
table, Spark manages both the metadata and the data in the file store.This could
be a local filesystem, HDFS, or an object store such as Amazon S3 or Azure Blob.
For an unmanaged table, Spark only manages the metadata, while you manage the data
yourself in an external data source such as Cassandra.

With a managed table, because Spark manages everything, a SQL command such as
DROP TABLE table_name deletes both the metadata and the data. With an unmanaged
table, the same command will delete only the metadata, not the actual data.


Creating SQL Databases and Tables

// In Scala/Python
spark.sql("CREATE DATABASE learn_spark_db")
spark.sql("USE learn_spark_db")

- Creating a managed table

// In Scala/Python
spark.sql("CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT,
distance INT, origin STRING, destination STRING)")

You can do the same thing using the DataFrame API like this:

# In Python
# Path to our US flight delays CSV file
csv_file = "/databricks-datasets/learning-spark-v2/flights/departuredelays.csv"
# Schema as defined in the preceding example
schema="date STRING, delay INT, distance INT, origin STRING, destination STRING"
flights_df = spark.read.csv(csv_file, schema=schema)
flights_df.write.saveAsTable("managed_us_delay_flights_tbl")

- Creating an unmanaged table

To create an unmanaged table from a data source such as a CSV file, in SQL use:
spark.sql("""CREATE TABLE us_delay_flights_tbl(date STRING, delay INT,
distance INT, origin STRING, destination STRING)
USING csv OPTIONS (PATH
'/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')""")

And within the DataFrame API use:

(flights_df
.write
.option("path", "/tmp/data/us_flights_delay")
.saveAsTable("us_delay_flights_tbl"))

- Creating Views

In addition to creating tables, Spark can create views on top of existing tables. Views
can be global (visible across all SparkSessions on a given cluster) or session-scoped
(visible only to a single SparkSession), and they are temporary: they disappear after
your Spark application terminates.

# In Python
df_sfo = spark.sql("SELECT date, delay, origin, destination FROM
us_delay_flights_tbl WHERE origin = 'SFO'")
df_jfk = spark.sql("SELECT date, delay, origin, destination FROM
us_delay_flights_tbl WHERE origin = 'JFK'")

# Create a temporary and global temporary view
df_sfo.createOrReplaceGlobalTempView("us_origin_airport_SFO_global_tmp_view")
df_jfk.createOrReplaceTempView("us_origin_airport_JFK_tmp_view")

Temporary views versus global temporary views

The difference between temporary and global temporary views being subtle, it can be a
source of mild confusion among developers new to Spark. A temporary view is tied
to a single SparkSession within a Spark application. In contrast, a global temporary
view is visible across multiple SparkSessions within a Spark application.

- Viewing the Metadata

// In Scala/Python
spark.catalog.listDatabases()
spark.catalog.listTables()
spark.catalog.listColumns("us_delay_flights_tbl")

- Caching SQL Tables

-- In SQL
CACHE [LAZY] TABLE <table-name>
UNCACHE TABLE <table-name>

- Reading Tables into DataFrames

// In Scala
val usFlightsDF = spark.sql("SELECT * FROM us_delay_flights_tbl")
val usFlightsDF2 = spark.table("us_delay_flights_tbl")

# In Python
us_flights_df = spark.sql("SELECT * FROM us_delay_flights_tbl")
us_flights_df2 = spark.table("us_delay_flights_tbl")


Data Sources for DataFrames and SQL Tables

- DataFrameReader

DataFrameReader.format(args).option("key", "value").schema(args).load()

To get an instance handle
to it, use:

SparkSession.read
// or
SparkSession.readStream

While read returns a handle to DataFrameReader to read into a DataFrame from a
static data source, readStream returns an instance to read from a streaming source.

- DataFrameWriter

DataFrameWriter does the reverse of its counterpart: it saves or writes data to a specified
built-in data source. Unlike with DataFrameReader, you access its instance not
from a SparkSession but from the DataFrame you wish to save.

DataFrameWriter.format(args)
.option(args)
.bucketBy(args)
.partitionBy(args)
.save(path)

DataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)

To get an instance handle, use:
DataFrame.write
// or
DataFrame.writeStream

- Parquet

- Reading Parquet files into a DataFrame:

Parquet files are stored in a directory structure that contains the data files, metadata,
a number of compressed files, and some status files.

To read Parquet files into a DataFrame, you simply specify the format and path:

// In Scala
val file = """/databricks-datasets/learning-spark-v2/flights/summary-data/
parquet/2010-summary.parquet/"""
val df = spark.read.format("parquet").load(file)

# In Python
file = """/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/
2010-summary.parquet/"""
df = spark.read.format("parquet").load(file)

Unless you are reading from a streaming data source there’s no need to supply the
schema, because Parquet saves it as part of its metadata.

- Reading Parquet files into a Spark SQL table:

-- In SQL
CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl
USING parquet
OPTIONS (
path "/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/
2010-summary.parquet/" )
Once you’ve created the table or view, you can read data into a DataFrame using SQL,
as we saw in some earlier examples:

// In Scala
spark.sql("SELECT * FROM us_delay_flights_tbl").show()

# In Python
spark.sql("SELECT * FROM us_delay_flights_tbl").show()

- Writing DataFrames to Parquet files

// In Scala
df.write.format("parquet")
.mode("overwrite")
.option("compression", "snappy")
.save("/tmp/data/parquet/df_parquet")
# In Python
(df.write.format("parquet")
.mode("overwrite")
.option("compression", "snappy")
.save("/tmp/data/parquet/df_parquet"))

Recall that Parquet is the default file format. If you don’t include
the format() method, the DataFrame will still be saved as a Parquet
file.


- Writing DataFrames to Spark SQL tables:

Writing a DataFrame to a SQL table is as easy as writing to a file—just use saveAsTa
ble() instead of save().

// In Scala
df.write
.mode("overwrite")
.saveAsTable("us_delay_flights_tbl")
# In Python
(df.write
.mode("overwrite")
.saveAsTable("us_delay_flights_tbl"))


JSON

- Reading a JSON file into a DataFrame:

You can read a JSON file into a DataFrame the same way you did with Parquet—just
specify "json" in the format() method:

// In Scala
val file = "/databricks-datasets/learning-spark-v2/flights/summary-data/json/*"
val df = spark.read.format("json").load(file)

# In Python
file = "/databricks-datasets/learning-spark-v2/flights/summary-data/json/*"
df = spark.read.format("json").load(file)

- Reading a JSON file into a Spark SQL table
You can also create a SQL table from a JSON file just like you did with Parquet:

-- In SQL
CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl
USING json
OPTIONS (
path "/databricks-datasets/learning-spark-v2/flights/summary-data/json/*"
)

Once the table is created, you can read data into a DataFrame using SQL:

// In Scala/Python
spark.sql("SELECT * FROM us_delay_flights_tbl").show()

- Writing DataFrames to JSON files:

// In Scala
df.write.format("json")
.mode("overwrite")
.option("compression", "snappy")
.save("/tmp/data/json/df_json")

# In Python
(df.write.format("json")
.mode("overwrite")
.option("compression", "snappy")
.save("/tmp/data/json/df_json"))

CSV

- Reading a CSV file into a DataFrame:

// In Scala
val file = "/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*"
val schema = "DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT"
val df = spark.read.format("csv")
.schema(schema)
.option("header", "true")
.option("mode", "FAILFAST") // Exit if any errors
.option("nullValue", "") // Replace any null data with quotes
.load(file)

# In Python
file = "/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*"
schema = "DEST_COUNTRY_NAME STRING, ORIGIN_COUNTRY_NAME STRING, count INT"
df = (spark.read.format("csv")
.option("header", "true")
.schema(schema)
.option("mode", "FAILFAST") # Exit if any errors
.option("nullValue", "") # Replace any null data field with quotes
.load(file))

Reading a CSV file into a Spark SQL table:

-- In SQL
CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl
USING csv
OPTIONS (
path "/databricks-datasets/learning-spark-v2/flights/summary-data/csv/*",
header "true",
inferSchema "true",
mode "FAILFAST"
)

Once you’ve created the table, you can read data into a DataFrame using SQL as
before:

// In Scala/Python
spark.sql("SELECT * FROM us_delay_flights_tbl").show(10)

- Writing DataFrames to CSV files:

// In Scala
df.write.format("csv").mode("overwrite").save("/tmp/data/csv/df_csv")

# In Python
df.write.format("csv").mode("overwrite").save("/tmp/data/csv/df_csv")

Avro

Introduced in Spark 2.4 as a built-in data source, the Avro format is used, for example,
by Apache Kafka for message serializing and deserializing. It offers many benefits,
including direct mapping to JSON, speed and efficiency, and bindings available
for many programming languages.


ORC

As an additional optimized columnar file format, Spark 2.x supports a vectorized
ORC reader. Two Spark configurations dictate which ORC implementation to use.
When spark.sql.orc.impl is set to native and spark.sql.orc.enableVectorize
dReader is set to true, Spark uses the vectorized ORC reader. A vectorized reader
reads blocks of rows (often 1,024 per block) instead of one row at a time, streamlining
operations and reducing CPU usage for intensive operations like scans, filters, aggregations,
and joins.

Images

In Spark 2.4 the community introduced a new data source, image files, to support
deep learning and machine learning frameworks such as TensorFlow and PyTorch.

- Reading an image file into a DataFrame

// In Scala
import org.apache.spark.ml.source.image
val imageDir = "/databricks-datasets/learning-spark-v2/cctvVideos/train_images/"
val imagesDF = spark.read.format("image").load(imageDir)
imagesDF.printSchema
imagesDF.select("image.height", "image.width", "image.nChannels", "image.mode",
"label").show(5, false)

# In Python
from pyspark.ml import image
image_dir = "/databricks-datasets/learning-spark-v2/cctvVideos/train_images/"
images_df = spark.read.format("image").load(image_dir)
images_df.printSchema()

Binary Files

- Reading a binary file into a DataFrame:

// In Scala
val path = "/databricks-datasets/learning-spark-v2/cctvVideos/train_images/"
val binaryFilesDF = spark.read.format("binaryFile")
.option("pathGlobFilter", "*.jpg")
.load(path)
binaryFilesDF.show(5)

# In Python
path = "/databricks-datasets/learning-spark-v2/cctvVideos/train_images/"
binary_files_df = (spark.read.format("binaryFile")
.option("pathGlobFilter", "*.jpg")
.load(path))
binary_files_df.show(5)

To ignore partitioning data discovery in a directory, you can set recursiveFile
Lookup to "true"

.option("recursiveFileLookup", "true")



******************************************************************
5.SPARK SQL AND DATAFRAMES: INTERACTING WITH EXTERNAL DATA SOURCES
******************************************************************

Spark SQL allows you to:

• Use user-defined functions for both Apache Hive and Apache Spark.
• Connect with external data sources such as JDBC and SQL databases, PostgreSQL,
MySQL, Tableau, Azure Cosmos DB, and MS SQL Server.
• Work with simple and complex types, higher-order functions, and common relational
operators.

Spark SQL and Apache Hive -> The current Spark SQL engine no longer uses the Hive code in its implementation.

User-Defined Functions

- Spark SQL UDFs:

// In Scala
// Create cubed function
val cubed = (s: Long) => {
s * s * s
}
// Register UDF
spark.udf.register("cubed", cubed)
// Create temporary view
spark.range(1, 9).createOrReplaceTempView("udf_test")

# In Python
from pyspark.sql.types import LongType
# Create cubed function
def cubed(s):
return s * s * s
# Register UDF
spark.udf.register("cubed", cubed, LongType())
# Generate temporary view
spark.range(1, 9).createOrReplaceTempView("udf_test")

You can now use Spark SQL to execute either of these cubed() functions:

// In Scala/Python
// Query the cubed UDF
spark.sql("SELECT id, cubed(id) AS id_cubed FROM udf_test").show()

Evaluation order and null checking in Spark SQL

Spark SQL (this includes SQL, the DataFrame API, and the Dataset API) does not
guarantee the order of evaluation of subexpressions.

1. Make the UDF itself null-aware and do null checking inside the UDF.
2. Use IF or CASE WHEN expressions to do the null check and invoke the UDF in a
conditional branch.

Speeding up and distributing PySpark UDFs with Pandas UDFs

One of the previous prevailing issues with using PySpark UDFs was that they had
slower performance than Scala UDFs. This was because the PySpark UDFs required
data movement between the JVM and Python.

From Apache Spark 3.0 with Python 3.6 and above, Pandas UDFs were split into two
API categories: Pandas UDFs and Pandas Function APIs.

- Pandas UDFs
With Apache Spark 3.0, Pandas UDFs infer the Pandas UDF type from Python
type hints in Pandas UDFs such as pandas

- Pandas Function APIs
Pandas Function APIs allow you to directly apply a local Python function to a
PySpark DataFrame where both the input and output are Pandas instances.

# In Python
# Import pandas
import pandas as pd
# Import various pyspark SQL functions including pandas_udf
from pyspark.sql.functions import col, pandas_udf
from pyspark.sql.types import LongType
# Declare the cubed function
def cubed(a: pd.Series) -> pd.Series:
return a * a * a
# Create the pandas UDF for the cubed function
cubed_udf = pandas_udf(cubed, returnType=LongType())

Querying with the Spark SQL Shell, Beeline, and Tableau

- Using the Spark SQL Shell

To start the Spark SQL CLI, execute the following command in the $SPARK_HOME
folder:

./bin/spark-sql

Create a table -> spark-sql> CREATE TABLE people (name STRING, age int);

Insert data into the table -> INSERT INTO people SELECT name, age FROM ...

Running a Spark SQL query -> spark-sql> SELECT * FROM people WHERE age < 20;

Working with Beeline

If you’ve worked with Apache Hive you may be familiar with the command-line tool
Beeline, a common utility for running HiveQL queries against HiveServer2. Beeline
is a JDBC client based on the SQLLine CLI. You can use this same utility to execute
Spark SQL queries against the Spark Thrift server.

Working with Tableau

Similar to running queries through Beeline or the Spark SQL CLI, you can connect
your favorite BI tool to Spark SQL via the Thrift JDBC/ODBC server.

External Data Sources

JDBC and SQL Databases

Spark SQL includes a data source API that can read data from other databases using
JDBC. It simplifies querying these data sources as it returns the results as a Data‐
Frame

To get started, you will need to specify the JDBC driver for your JDBC data source

./bin/spark-shell --driver-class-path $database.jar --jars $database.jar

The importance of partitioning

When transferring large amounts of data between Spark SQL and a JDBC external
source, it is important to partition your data source. All of your data is going through
one driver connection, which can saturate and significantly slow down the performance
of your extraction.

PostgreSQL

To connect to a PostgreSQL database, build or download the JDBC jar from Maven
and add it to your classpath. Then start a Spark shell (spark-shell or pyspark), specifying
that jar:

bin/spark-shell --jars postgresql-42.2.6.jar

// In Scala
// Read Option 1: Loading data from a JDBC source using load method
val jdbcDF1 = spark
.read
.format("jdbc")
.option("url", "jdbc:postgresql:[DBSERVER]")
.option("dbtable", "[SCHEMA].[TABLENAME]")
.option("user", "[USERNAME]")
.option("password", "[PASSWORD]")
.load()

// Read Option 2: Loading data from a JDBC source using jdbc method
// Create connection properties
import java.util.Properties
val cxnProp = new Properties()
cxnProp.put("user", "[USERNAME]")
cxnProp.put("password", "[PASSWORD]")
// Load data using the connection properties
val jdbcDF2 = spark
.read
.jdbc("jdbc:postgresql:[DBSERVER]", "[SCHEMA].[TABLENAME]", cxnProp)
// Write Option 1: Saving data to a JDBC source using save method
jdbcDF1
.write
.format("jdbc")
.option("url", "jdbc:postgresql:[DBSERVER]")
.option("dbtable", "[SCHEMA].[TABLENAME]")
.option("user", "[USERNAME]")
.option("password", "[PASSWORD]")
.save()
// Write Option 2: Saving data to a JDBC source using jdbc method
jdbcDF2.write
.jdbc(s"jdbc:postgresql:[DBSERVER]", "[SCHEMA].[TABLENAME]", cxnProp)


MySQL

To connect to a MySQL database, build or download the JDBC jar from Maven or
MySQL (the latter is easier!) and add it to your classpath. Then start a Spark shell
(spark-shell or pyspark), specifying that jar:

bin/spark-shell --jars mysql-connector-java_8.0.16-bin.jar


Azure Cosmos DB

To connect to an Azure Cosmos DB database, build or download the JDBC jar from
Maven or GitHub and add it to your classpath. Then start a Scala or PySpark shell,
specifying this jar

bin/spark-shell --jars azure-cosmosdb-spark_2.4.0_2.11-1.3.5-uber.jar


MS SQL Server

To connect to an MS SQL Server database, download the JDBC jar and add it to your
classpath. Then start a Scala or PySpark shell, specifying this jar:
bin/spark-shell --jars mssql-jdbc-7.2.2.jre8.jar


Other External Sources
There are just some of the many external data sources Apache Spark can connect to;
other popular data sources include:

• Apache Cassandra
• Snowflake
• MongoDB


Higher-Order Functions in DataFrames and Spark SQL

There are two typical solutions for manipulating complex
data types:

• Exploding the nested structure into individual rows, applying some function, and
then re-creating the nested structure

• Building a user-defined function


Option 1: Explode and Collect

-- In SQL
SELECT id, collect_list(value + 1) AS values
FROM (SELECT id, EXPLODE(values) AS value
FROM table) x
GROUP BY id

Option 2: User-Defined Function

spark.sql("SELECT id, plusOneInt(values) AS values FROM table").show()


Built-in Functions for Complex Data Types

Instead of using these potentially expensive techniques, you may be able to use some
of the built-in functions for complex data types included as part of Apache Spark 2.4
and later.


Higher-Order Functions

-- In SQL
transform(values, value -> lambda expression)

The transform() function takes an array (values) and anonymous function (lambda
expression) as input. The function transparently creates a new array by applying the
anonymous function to each element, and then assigning the result to the output
array (similar to the UDF approach, but more efficiently).

Let’s create a sample data set so we can run some examples:

# In Python
from pyspark.sql.types import *
schema = StructType([StructField("celsius", ArrayType(IntegerType()))])
t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]
t_c = spark.createDataFrame(t_list, schema)
t_c.createOrReplaceTempView("tC")
# Show the DataFrame
t_c.show()

// In Scala
// Create DataFrame with two rows of two arrays (tempc1, tempc2)
val t1 = Array(35, 36, 32, 30, 40, 42, 38)
val t2 = Array(31, 32, 34, 55, 56)
val tC = Seq(t1, t2).toDF("celsius")
tC.createOrReplaceTempView("tC")
// Show the DataFrame
tC.show()

With the preceding DataFrame you can run the following higher-order function
queries.

- transform()

transform(array<T>, function<T, U>): array<U>

The transform() function produces an array by applying a function to each element
of the input array

- filter()

filter(array<T>, function<T, Boolean>): array<T>

The filter() function produces an array consisting of only the elements of the input
array for which the Boolean function is true

- exists()

exists(array<T>, function<T, V, Boolean>): Boolean

The exists() function returns true if the Boolean function holds for any element in
the input array

reduce()

reduce(array<T>, B, function<B, T, B>, function<B, R>)

The reduce() function reduces the elements of the array to a single value by merging
the elements into a buffer B using function<B, T, B> and applying a finishing
function<B, R> on the final buffer

Common DataFrames and Spark SQL Operations

Part of the power of Spark SQL comes from the wide range of DataFrame operations
(also known as untyped Dataset operations) it supports. The list of operations is quite
extensive and includes:

• Aggregate functions
• Collection functions
• Datetime functions
• Math functions
• Miscellaneous functions
• Non-aggregate functions
• Sorting functions
• String functions
• UDF functions
• Window functions

We will focus on the following common relational operations:

• Unions and joins
• Windowing
• Modifications

- Unions

A common pattern within Apache Spark is to union two different DataFrames with
the same schema together. This can be achieved using the union() method:

// Scala
// Union two tables
val bar = delays.union(foo)

# In Python
# Union two tables
bar = departureDelays.union(foo)

- Joins

A common DataFrame operation is to join two DataFrames (or tables) together. By
default, a Spark SQL join is an inner join, with the options being inner, cross,
outer, full, full_outer, left, left_outer, right, right_outer, left_semi, and
left_anti.


foo.join(
airports.as('air),
$"air.IATA" === $"origin"
).select("City", "State", "date", "delay", "distance", "destination").show()

- Windowing

A window function uses values from the rows in a window (a range of input rows) to
return a set of values, typically in the form of another row. With window functions, it
is possible to operate on a group of rows while still returning a single value for every
input row.

use a window function like dense_rank() to perform
the following calculation:

-- In SQL
spark.sql("""
SELECT origin, destination, TotalDelays, rank
FROM (
SELECT origin, destination, TotalDelays, dense_rank()
OVER (PARTITION BY origin ORDER BY TotalDelays DESC) as rank
FROM departureDelaysWindow
) t
WHERE rank <= 3
""").show()

- Modifications

Another common operation is to perform modifications to the DataFrame. While
DataFrames themselves are immutable, you can modify them through operations that
create new, different DataFrames, with different columns, for example.

- Adding new columns
To add a new column to the foo DataFrame, use the withColumn() method

- Dropping columns
To drop a column, use the drop() method

- Renaming columns
You can rename a column using the rename() method

Pivoting
When working with your data, sometimes you will need to swap the columns for the
rows—i.e., pivot your data.

-- In SQL
SELECT * FROM (
SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay
FROM departureDelays WHERE origin = 'SEA'
)
PIVOT (
CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay, MAX(delay) AS MaxDelay
FOR month IN (1 JAN, 2 FEB)
)
ORDER BY destination


**************************
6. SPARK SQL AND DATASETS
**************************

Datasets—strongly typed distributed collections

We’ll explore working with Datasets in Java and Scala, how Spark manages memory to accommodate Dataset
constructs as part of the high-level API, and the costs associated with using Datasets.

Single API for Java and Scala

Among the languages supported by Spark, only Scala and Java are strongly typed; hence, 
Python and R support only the untyped DataFrame API.

Scala Case Classes and JavaBeans for Datasets

In order to create Dataset[T], where T is your typed object in Scala, you need a case
class that defines the object.

// In Scala
case class Bloggers(id:Int, first:String, last:String, url:String, date:String,
hits: Int, campaigns:Array[String])

Similarly, you can create a JavaBean class of type Bloggers in Java and then use
encoders to create a Dataset<Bloggers>:

// In Java
import org.apache.spark.sql.Encoders;
import java.io.Serializable;
public class Bloggers implements Serializable {
private int id;
private String first;
private String last;
private String url;
private String date;
private int hits;
private Array[String] campaigns;


// JavaBean getters and setters
int getID() { return id; }
void setID(int i) { id = i; }
String getFirst() { return first; }
void setFirst(String f) { first = f; }
String getLast() { return last; }
void setLast(String l) { last = l; }
String getURL() { return url; }
void setURL (String u) { url = u; }
String getDate() { return date; }
Void setDate(String d) { date = d; }
int getHits() { return hits; }
void setHits(int h) { hits = h; }
Array[String] getCampaigns() { return campaigns; }
void setCampaigns(Array[String] c) { campaigns = c; }
}


// Create Encoder
Encoder<Bloggers> BloggerEncoder = Encoders.bean(Bloggers.class);
String bloggers = "../bloggers.json"
Dataset<Bloggers>bloggersDS = spark
.read
.format("json")
.option("path", bloggers)
.load()
.as(BloggerEncoder);

The Dataset API requires that you define your data types ahead of time and
that your case class or JavaBean class matches your schema.

The names of the fields in the Scala case class or Java class definition
must match the order in the data source. The column names
for each row in the data are automatically mapped to the corresponding
names in the class and the types are automatically
preserved.

- Creating Sample Data

// In Scala
import scala.util.Random._
// Our case class for the Dataset
case class Usage(uid:Int, uname:String, usage: Int)
val r = new scala.util.Random(42)
// Create 1000 instances of scala Usage class
// This generates data on the fly
val data = for (i <- 0 to 1000)
yield (Usage(i, "user-" + r.alphanumeric.take(5).mkString(""),
r.nextInt(1000)))
// Create a Dataset of Usage typed data
val dsUsage = spark.createDataset(data)
dsUsage.show(10)

- Transforming Sample Data

Higher-order functions and functional programming

// In Scala
import org.apache.spark.sql.functions._
dsUsage
.filter(d => d.usage > 900)
.orderBy(desc("usage"))
.show(5, false)

Another way is to define a function and supply that function as an argument to
filter():

def filterWithUsage(u: Usage) = u.usage > 900
dsUsage.filter(filterWithUsage(_)).orderBy(desc("usage")).show(5)

In the first case we used a lambda expression, {d.usage > 900}, as an argument to
the filter() method, whereas in the second case we defined a Scala function, def
filterWithUsage(u: Usage) = u.usage > 900.

// In Java
// Define a Java filter function
FilterFunction<Usage> f = new FilterFunction<Usage>() {
public boolean call(Usage u) {
return (u.usage > 900);
}
};

To use map() in Java, you have to define a MapFunction<T>.

// In Java
// Define an inline MapFunction
dsUsage.map((MapFunction<Usage, Double>) u -> {
if (u.usage > 750)
return u.usage * 0.15;
else
return u.usage * 0.50;
}, Encoders.DOUBLE()).show(5);


- Converting DataFrames to Datasets

To convert an existing DataFrame df to a Dataset of type SomeCaseClass,
simply use the df.as[SomeCaseClass] notation.

Memory Management for Datasets and DataFrames

• Spark 2.x introduced the second-generation Tungsten engine, featuring wholestage
code generation and vectorized column-based memory layout. Built on
ideas and techniques from modern compilers, this new version also capitalized
on modern CPU and cache architectures for fast parallel data access with the
“single instruction, multiple data” (SIMD) approach.


Dataset Encoders

Encoders convert data in off-heap memory from Spark’s internal Tungsten format to
JVM Java objects. In other words, they serialize and deserialize Dataset objects from
Spark’s internal format to JVM objects


Spark’s Internal Format Versus Java Object Format

Instead of creating JVM-based objects for Datasets or DataFrames, Spark allocates
off-heap Java memory to lay out their data and employs encoders to convert the data
from in-memory representation to JVM object.


Serialization and Deserialization (SerDe)

A concept not new in distributed computing, where data frequently travels over the
network among computer nodes in a cluster, serialization and deserialization is the
process by which a typed object is encoded (serialized) into a binary presentation or
format by the sender and decoded (deserialized) from binary format into its respective
data-typed object by the receiver.

Costs of Using Datasets

When Datasets are passed to higher-order functions such as filter(), map(), or flatMap() 
that take lambdas and functional arguments, there is a cost associated with deserializing 
from Spark’s internal Tungsten format into the JVM object.

Compared to other serializers used before encoders were introduced in Spark, this
cost is minor and tolerable. However, over larger data sets and many queries, this cost
accrues and can affect performance.

- Strategies to Mitigate Costs

One strategy to mitigate excessive serialization and deserialization is to use
DSL expressions in your queries and avoid excessive use of lambdas as anonymous
functions as arguments to higher-order functions. Because lambdas are anonymous
and opaque to the Catalyst optimizer until runtime, when you use them it cannot efficiently
discern what you’re doing (you’re not telling Spark what to do) and thus cannot
optimize your queries

The second strategy is to chain your queries together in such a way that serialization
and deserialization is minimized. Chaining queries together is a common practice in
Spark.


*******************************************
7. OPTIMIZING AND TUNING SPARK APPLICATIONS
*******************************************


Optimizing and Tuning Spark for Efficiency

Viewing and Setting Apache Spark Configurations

There are three ways you can get and set Spark properties. 

- The first is through a set of
configuration files. In your deployment’s $SPARK_HOME directory (where you installed
Spark), there are a number of config files: conf/spark-defaults.conf.template, conf/
log4j.properties.template, and conf/spark-env.sh.template. Changing the default values
in these files and saving them without the .template suffix instructs Spark to use these
new values.

- The second way is to specify Spark configurations directly in your Spark application
or on the command line when submitting the application with spark-submit, using
the --conf flag


- The third option is through a programmatic interface via the Spark shell. As with
everything else in Spark, APIs are the primary method of interaction. Through the
SparkSession object, you can access most Spark config settings.

// In Scala
// mconf is a Map[String, String]
scala> val mconf = spark.conf.getAll


Scaling Spark for Large Workloads

Static versus dynamic resource allocation
When you specify compute resources as command-line arguments to spark-submit,
as we did earlier, you cap the limit. This means that if more resources are needed later
as tasks queue up in the driver due to a larger than anticipated workload, Spark cannot
accommodate or allocate extra resources.

If instead you use Spark’s dynamic resource allocation configuration, the Spark driver
can request more or fewer compute resources as the demand of large workloads flows
and ebbs.

In scenarios where your workloads are dynamic—that is, they vary in their
demand for compute capacity—using dynamic allocation helps to accommodate sudden
peaks. One use case where this can be helpful is streaming, where the data flow volume may
be uneven.

Another is on-demand data analytics, where you might have a high volume
of SQL queries during peak hours.

To enable and configure dynamic allocation, you can use settings like the following.

spark.dynamicAllocation.enabled true
spark.dynamicAllocation.minExecutors 2
spark.dynamicAllocation.schedulerBacklogTimeout 1m
spark.dynamicAllocation.maxExecutors 20
spark.dynamicAllocation.executorIdleTimeout 2min

Configuring Spark executors’ memory and the shuffle service

The amount of memory available to each executor is controlled by
spark.executor.memory. This is divided into three sections:
execution memory, storage memory, and reserved memory

Maximizing Spark parallelism

How partitions are created. As mentioned previously, Spark’s tasks process data as partitions
read from disk into memory. The size of a partition in Spark is dictated by spark.sql.files.maxPartitionBytes.

Partitions are also created when you explicitly use certain methods of the DataFrame
API. For example, while creating a large DataFrame or reading a large file from disk,
you can explicitly instruct Spark to create a certain number of partitions:

// In Scala
val ds = spark.read.textFile("../README.md").repartition(16)
ds: org.apache.spark.sql.Dataset[String] = [value: string]
ds.rdd.getNumPartitions
res5: Int = 16
val numDF = spark.range(1000L * 1000 * 1000).repartition(16)
numDF.rdd.getNumPartitions
numDF: org.apache.spark.sql.Dataset[Long] = [id: bigint]
res12: Int = 16


Caching and Persistence of Data

What is the difference between caching and persistence? In Spark they are synonymous.
Two API calls, cache() and persist(), offer these capabilities

DataFrame.cache()

cache() will store as many of the partitions read in memory across Spark executors
as memory allows


// In Scala
// Create a DataFrame with 10M records
val df = spark.range(1 * 10000000).toDF("id").withColumn("square", $"id" * $"id")
df.cache() // Cache the data
df.count() // Materialize the cache
res3: Long = 10000000
Command took 5.11 seconds
df.count() // Now get it from the cache
res4: Long = 10000000
Command took 0.44 seconds

The first count() materializes the cache, whereas the second one accesses the cache,
resulting in a close to 12 times faster access time for this data set.

DataFrame.persist()

persist(StorageLevel.LEVEL) is nuanced, providing control over how your data is
cached via StorageLevel

The data is persisted on disk, not in memory. To
unpersist your cached data, just call DataFrame.unpersist().

You can also cache the tables or
views derived from DataFrames. This gives them more readable names in the Spark
UI. For example:
// In Scala
df.createOrReplaceTempView("dfTable")
spark.sql("CACHE TABLE dfTable")
spark.sql("SELECT count(*) FROM dfTable").show()


When to Cache and Persist

• DataFrames commonly used during iterative machine learning training
• DataFrames accessed commonly for doing frequent transformations during ETL
or building data pipelines

When Not to Cache and Persist

• DataFrames that are too big to fit in memory
• An inexpensive transformation on a DataFrame not requiring frequent use,
regardless of size

A Family of Spark Joins

Join operations are a common type of transformation in big data analytics in which
two data sets, in the form of tables or DataFrames, are merged over a common
matching key. Similar to relational databases, the Spark DataFrame and Dataset APIs
and Spark SQL offer a series of join transformations: inner joins, outer joins, left
joins, right joins, etc.

Spark has five distinct join strategies by which it exchanges, moves, sorts, groups, and
merges data across executors: the broadcast hash join (BHJ), shuffle hash join (SHJ),
shuffle sort merge join (SMJ), broadcast nested loop join (BNLJ), and shuffle-andreplicated
nested loop join (a.k.a. Cartesian product join).

- Broadcast Hash Join

Also known as a map-side-only join, the broadcast hash join is employed when two
data sets, one small (fitting in the driver’s and executor’s memory) and another large
enough to ideally be spared from movement, need to be joined over certain conditions
or columns.

When to use a broadcast hash join

Use this type of join under the following conditions for maximum benefit:

• When each key within the smaller and larger data sets is hashed to the same partition
by Spark
• When one data set is much smaller than the other (and within the default config
of 10 MB, or more if you have sufficient memory)
• When you only want to perform an equi-join, to combine two data sets based on
matching unsorted keys
• When you are not worried by excessive network bandwidth usage or OOM
errors, because the smaller data set will be broadcast to all Spark executors

- Shuffle Sort Merge Join

The sort-merge algorithm is an efficient way to merge two large data sets over a common
key that is sortable, unique, and can be assigned to or stored in the same partition—
that is, two data sets with a common hashable key that end up being on the
same partition.

When to use a shuffle sort merge join

Use this type of join under the following conditions for maximum benefit:

• When each key within two large data sets can be sorted and hashed to the same
partition by Spark
• When you want to perform only equi-joins to combine two data sets based on
matching sorted keys
• When you want to prevent Exchange and Sort operations to save large shuffles
across the network

Inspecting the Spark UI

A spark-submit job will launch the Spark UI, and you can connect to it on the local
host (in local mode) or through the Spark driver (in other modes) at the default port
4040.

- Jobs and Stages
- Storage
- Environment
- Executors
- SQL


*************************
8. STRUCTURED STREAMING
*************************

Often data arrives continuously and needs to be processed in a real-time manner

Evolution of the Apache Spark Stream Processing Engine

Stream processing is defined as the continuous processing of endless streams of data.
With the advent of big data, stream processing systems transitioned from single-node
processing engines to multiple-node, distributed processing engines. Traditionally,
distributed stream processing has been implemented with a record-at-a-time processing
model

This processing model can achieve very low latencies—that is, an input record can be processed by
the pipeline and the resulting output can be generated within milliseconds. However,
this model is not very efficient at recovering from node failures and straggler nodes
(i.e., nodes that are slower than others); it can either recover from a failure very fast
with a lot of extra failover resources.

The Advent of Micro-Batch Stream Processing

This traditional approach was challenged by Apache Spark when it introduced Spark
Streaming (also called DStreams). It introduced the idea of micro-batch stream processing,
where the streaming computation is modeled as a continuous series of small,
map/reduce-style batch processing jobs (hence, “micro-batches”) on small chunks of
the stream data.

• Spark’s agile task scheduling can very quickly and efficiently recover from failures
and straggler executors by rescheduling one or more copies of the tasks on any of
the other executors.

• The deterministic nature of the tasks ensures that the output data is the same no
matter how many times the task is reexecuted.

Furthermore, the DStream API was built upon Spark’s batch RDD API. Therefore,
DStreams had the same functional semantics and fault-tolerance model as RDDs.

• Lessons Learned from Spark Streaming (DStreams)

- Lack of a single API for batch and stream processing

- Lack of separation between logical and physical plans

- Lack of native support for event-time windows

The Programming Model of Structured Streaming

“Table” is a well-known concept that developers are familiar with when building
batch applications. Structured Streaming extends this concept to streaming applications
by treating a stream as an unbounded, continuously appended table

Every new record received in the data stream is like a new row being appended to the
unbounded input table.

Structured Streaming provides three output modes:

- Append mode ->Only the new rows appended to the result table since the last trigger will be written
to the external storage
- Update mode -> Only the rows that were updated in the result table since the last trigger will be
changed in the external storage
- Complete mode -> The entire updated result table will be written to external storage

The Fundamentals of a Structured Streaming Query

Five Steps to Define a Streaming Query

- Step 1: Define input sources
- Step 2: Transform data
- Step 3: Define output sink and output mode
- Step 4: Specify processing details
- Step 5: Start the query

Under the Hood of an Active Streaming Query

1. Spark SQL analyzes and optimizes this logical plan to ensure that it can be executed
incrementally and efficiently on streaming data.
2. Spark SQL starts a background thread that continuously executes the following
loop
3. This loop continues until the query is terminated,

Recovering from Failures with Exactly-Once Guarantees

As a final note regarding restarting queries, it is possible to make minor modifications
to a query between restarts

- DataFrame transformations
- Source and sink options
- Processing details

Monitoring an Active Query

- Querying current status using StreamingQuery
- Get current metrics using StreamingQuery
- Get current status using StreamingQuery.status()
- Publishing metrics using Dropwizard Metrics
- Publishing metrics using custom StreamingQueryListeners

Streaming Data Sources and Sinks

How to use the built-in streaming data sources and sinks

Files

- Reading from files

Structured Streaming can treat files written into a directory as a data stream.

# In Python
from pyspark.sql.types import *
inputDirectoryOfJsonFiles = ...
fileSchema = (StructType()
.add(StructField("key", IntegerType()))
.add(StructField("value", IntegerType())))
inputDF = (spark
.readStream
.format("json")
.schema(fileSchema)
.load(inputDirectoryOfJsonFiles))

// In Scala
import org.apache.spark.sql.types._
val inputDirectoryOfJsonFiles = ...
val fileSchema = new StructType()
.add("key", IntegerType)
.add("value", IntegerType)
val inputDF = spark.readStream
.format("json")
.schema(fileSchema)
.load(inputDirectoryOfJsonFiles)
The returned streaming DataFrame will


- Writing to files

Structured Streaming supports writing streaming query output to files in the same
formats as reads. However, it only supports append mode, because while it is easy to
write new files in the output directory (i.e., append data to a directory), it is hard to
modify existing data files

# In Python
outputDir = ...
checkpointDir = ...
resultDF = ...
streamingQuery = (resultDF.writeStream
.format("parquet")
.option("path", outputDir)
.option("checkpointLocation", checkpointDir)
.start())

Apache Kafka

Apache Kafka is a popular publish/subscribe system that is widely used for storage of
data streams. Structured Streaming has built-in support for reading from and writing
to Apache Kafka.

- Reading from Kafka

# In Python
inputDF = (spark
.readStream
.format("kafka")
.option("kafka.bootstrap.servers", "host1:port1,host2:port2")
.option("subscribe", "events")
.load())

- Writing to Kafka

For writing to Kafka, Structured Streaming expects the result DataFrame to have a
few columns of specific names and types

You can write to Kafka in all three output modes, though complete mode is not recommended
as it will repeatedly output the same records

Custom Streaming Sources and Sinks

In this section, we will discuss how to read and write to storage systems that do not
have built-in support in Structured Streaming. In particular, you’ll see how to use the
foreachBatch() and foreach()

Writing to any storage system -> foreachBatch() and foreach()


- Using foreachBatch()

# In Python
hostAddr = "<ip address>"
keyspaceName = "<keyspace>"
tableName = "<tableName>"
spark.conf.set("spark.cassandra.connection.host", hostAddr)
def writeCountsToCassandra(updatedCountsDF, batchId):
# Use Cassandra batch data source to write the updated counts
(updatedCountsDF
.write
.format("org.apache.spark.sql.cassandra")
.mode("append")
.options(table=tableName, keyspace=keyspaceName)
.save())
streamingQuery = (counts
.writeStream
.foreachBatch(writeCountsToCassandra)
.outputMode("update")
.option("checkpointLocation", checkpointDir)
.start())

With foreachBatch(), you can do the following:

- Reuse existing batch data sources
- Write to multiple locations
- Apply additional DataFrame operations


- Using foreach()

You can express the data-writing logic by dividing it into three methods: open(), process(), and close().


Data Transformations

Only the DataFrame operations that can be executed incrementally are supported
in Structured Streaming. These operations are broadly classified into stateless and stateful operations.

Incremental Execution and Streaming State

Each execution is considered as a micro-batch, and the partial intermediate result
that is communicated between the executions is called the streaming “state.”

Stateless Transformations

All projection operations (e.g., select(), explode(), map(), flatMap()) and selection
operations (e.g., filter(), where()) process each input record individually
without needing any information from previous rows. This lack of dependence on
prior input data makes them stateless operations.

Stateful Transformations

The simplest example of a stateful transformation is DataFrame.groupBy().count(),
which generates a running count of the number of records received since the beginning
of the query. In every micro-batch, the incremental plan adds the count of new
records to the previous count generated by the previous micro-batch.


Distributed and fault-tolerant state management

For stateful stream processing queries, besides writing to sinks, each
micro-batch of tasks generates intermediate state data which will be consumed by the
next micro-batch. This state data generation is completely partitioned and distributed
(as all reading, writing, and processing is in Spark), and it is cached in the executor
memory for efficient consumption.

Types of stateful operations

- Managed stateful operations -> These automatically identify and clean up old state, based on an operationspecific
definition of “old.”

- Unmanaged stateful operations -> These operations let you define your own custom state cleanup logic.

• MapGroupsWithState
• FlatMapGroupsWithState

Stateful Streaming Aggregations

Structured Streaming can incrementally execute most DataFrame aggregation operations.
You can aggregate data by keys (e.g., streaming word count) and/or by time
(e.g., count records received every hour).

Aggregations Not Based on Time

- Global aggregations -> Aggregations across all the data in the stream

# In Python
runningCount = sensorReadings.groupBy().count()

// In Scala
val runningCount = sensorReadings.groupBy().count()


You cannot use direct aggregation operations like Data Frame.count() and Dataset.reduce() on streaming DataFrames.
This is because, for static DataFrames, these operations immediately return the final computed aggregates,
whereas for streaming DataFrames the aggregates have to be continuously updated. Therefore, you have to always use Data
Frame.groupBy() or Dataset.groupByKey() for aggregations on streaming DataFrames.

- Grouped aggregations -> Aggregations within each group or key present in the data stream.

# In Python
baselineValues = sensorReadings.groupBy("sensorId").mean("value")

// In Scala
val baselineValues = sensorReadings.groupBy("sensorId").mean("value")

Besides counts and averages, streaming DataFrames support the following types of
aggregations (similar to batch DataFrames):

- All built-in aggregation functions

sum(), mean(), stddev(), countDistinct(), collect_set(), approx_count_dis
tinct(), etc.


- Multiple aggregations computed together

You can apply multiple aggregation functions to be computed together in the following
manner:

# In Python
from pyspark.sql.functions import *
multipleAggs = (sensorReadings
.groupBy("sensorId")
.agg(count("*"), mean("value").alias("baselineValue"),
collect_set("errorCode").alias("allErrorCodes")))

// In Scala
import org.apache.spark.sql.functions.*
val multipleAggs = sensorReadings
.groupBy("sensorId")
.agg(count("*"), mean("value").alias("baselineValue"),
collect_set("errorCode").alias("allErrorCodes"))


- User-defined aggregation functions

Aggregations with Event-Time Windows

In many cases, rather than running aggregations over the whole stream, you want
aggregations over data bucketed by time windows.

# In Python
from pyspark.sql.functions import *
(sensorReadings
.groupBy("sensorId", window("eventTime", "5 minute"))
.count())

// In Scala
import org.apache.spark.sql.functions.*
sensorReadings
.groupBy("sensorId", window("eventTime", "5 minute"))
.count()

The key thing to note here is the window() function, which allows us to express the
five-minute windows as a dynamically computed grouping column.

For example, if you want to compute counts corresponding to 10-minute
windows sliding every 5 minutes, then you can do the following:

# In Python
(sensorReadings
.groupBy("sensorId", window("eventTime", "10 minute", "5 minute"))
.count())

// In Scala
sensorReadings
.groupBy("sensorId", window("eventTime", "10 minute", "5 minute"))
.count()


Handling late data with watermarks

A watermark is defined as a moving threshold in event time that trails behind the
maximum event time seen by the query in the processed data. The trailing gap,
known as the watermark delay, defines how long the engine will wait for late data to
arrive. By knowing the point at which no more data will arrive for a given group, the
engine can automatically finalize the aggregates of certain groups and drop them
from the state.This limits the total amount of state that the engine has to maintain to
compute the results of the query.

# In Python
(sensorReadings
.withWatermark("eventTime", "10 minutes")
.groupBy("sensorId", window("eventTime", "10 minutes", "5 minutes"))
.mean("value"))

Note that you must call withWatermark() before the groupBy() and on the same
timestamp column as that used to define windows.

- Semantic guarantees with watermarks.

Before we conclude this section about watermarks,let’s consider the precise semantic
guarantee that watermarking provides. A watermark of 10 minutes guarantees that the engine
will never drop any data that is delayed by less than 10 minutes compared to the 
latest event time seen in the input data. However, the guarantee is strict only in 
one direction. Data delayed by more than 10 minutes is not guaranteed to be dropped—that is,
it may get aggregated. Whether an input record more than 10 minutes late will actually
be aggregated or not depends on the exact timing of when the record was received and 
when the microbatch processing it was triggered.


Supported output modes

- Update mode -> In this mode, every micro-batch will output only the rows where the aggregate got updated.
- Complete mode -> In this mode, every micro-batch will output all the updated aggregates, irrespective
of their age or whether they contain changes.
- Append mode -> This mode can be used only with aggregations on event-time windows and with
watermarking enabled.


Streaming Joins

- Stream–Static Joins

# In Python
matched = clicksStream.join(impressionsStatic, "adId")

// In Scala
val matched = clicksStream.join(impressionsStatic, "adId")

Besides inner joins, Structured Streaming also supports two types of stream–static
outer joins:

• Left outer join when the left side is a streaming DataFrame
• Right outer join when the right side is a streaming DataFrames

The other kinds of outer joins (e.g., full outer and left outer with a streaming DataFrame on the 
right) are not supported because they are not easy to run incrementally.

# In Python
matched = clicksStream.join(impressionsStatic, "adId", "leftOuter")

// In Scala
val matched = clicksStream.join(impressionsStatic, Seq("adId"), "leftOuter")

There are a few key points to note about stream–static joins:

• Stream–static joins are stateless operations, and therefore do not require any kind
of watermarking.
• The static DataFrame is read repeatedly while joining with the streaming data of
every micro-batch, so you can cache the static DataFrame to speed up the reads.
• If the underlying data in the data source on which the static DataFrame was
defined changes, whether those changes are seen by the streaming query depends
on the specific behavior of the data source. For example, if the static DataFrame
was defined on files, then changes to those files (e.g., appends) will not be picked
up until the streaming query is restarted.


- Stream–Stream Joins

The challenge of generating joins between two data streams is that, at any point in
time, the view of either Dataset is incomplete, making it much harder to find matches
between inputs. The matching events from the two streams may arrive in any order
and may be arbitrarily delayed.


Inner joins with optional watermarking

To get the stream of matching impressions and their corresponding clicks, we can use
the same code we used earlier for static joins and stream–static joins:

# In Python
# Streaming DataFrame [adId: String, impressionTime: Timestamp, ...]
impressions = spark.readStream. ...

# Streaming DataFrame[adId: String, clickTime: Timestamp, ...]
clicks = spark.readStream. ...
matched = impressions.join(clicks, "adId")

// In Scala
// Streaming DataFrame [adId: String, impressionTime: Timestamp, ...]
val impressions = spark.readStream. ...

// Streaming DataFrame[adId: String, clickTime: Timestamp, ...]
val clicks = spark.readStream. ...
val matched = impressions.join(clicks, "adId")

Even though the code is the same, the execution is completely different. When this
query is executed, the processing engine will recognize it to be a stream–stream join
instead of a stream–static join.


However, in this query, we have not given any indication of how long the engine
should buffer an event to find a match. Therefore, the engine may buffer an event forever
and accumulate an unbounded amount of streaming state. To limit the streaming
state maintained by stream–stream joins, you need to know the following
information about your use case:

• What is the maximum time range between the generation of the two events at their
respective sources? In the context of our use case, let’s assume that a click can
occur within zero seconds to one hour after the corresponding impression.
• What is the maximum duration an event can be delayed in transit between the
source and the processing engine? For example, ad clicks from a browser may get
delayed due to intermittent connectivity and arrive much later than expected,
and out of order. Let’s say that impressions and clicks can be delayed by at most
two and three hours, respectively.

These delay limits and event-time constraints can be encoded in the DataFrame operations
using watermarks and time range conditions. In other words, you will have to
do the following additional steps in the join to ensure state cleanup:

1. Define watermark delays on both inputs, such that the engine knows how
delayed the input can be (similar to with streaming aggregations).

2. Define a constraint on event time across the two inputs, such that the engine can
figure out when old rows of one input are not going to be required (i.e., will not
satisfy the time constraint) for matches w

a. Time range join conditions (e.g., join condition = "leftTime BETWEEN
rightTime AND rightTime + INTERVAL 1 HOUR")
b. Join on event-time windows (e.g., join condition = "leftTimeWindow =
rightTimeWindow")

# In Python
# Define watermarks
impressionsWithWatermark = (impressions
.selectExpr("adId AS impressionAdId", "impressionTime")
.withWatermark("impressionTime", "2 hours"))

clicksWithWatermark = (clicks
.selectExpr("adId AS clickAdId", "clickTime")
.withWatermark("clickTime", "3 hours"))

# Inner join with time range conditions
(impressionsWithWatermark.join(clicksWithWatermark,
expr("""
clickAdId = impressionAdId AND
clickTime BETWEEN impressionTime AND impressionTime + interval 1 hour""")))

There are a few key points to remember about inner joins:

• For inner joins, specifying watermarking and event-time constraints are both
optional.

• Similar to the guarantees provided by watermarking on aggregations, a watermark
delay of two hours guarantees that the engine will never drop or not match
any data that is less than two hours delayed, but data delayed by more than two
hours may or may not get processed.

- Outer joins with watermarking

# In Python
# Left outer join with time range conditions
(impressionsWithWatermark.join(clicksWithWatermark,
expr("""
clickAdId = impressionAdId AND
clickTime BETWEEN impressionTime AND impressionTime + interval 1 hour"""),
"leftOuter")) # only change: set the outer join type

However, there are a few additional
points to note about outer joins:

• Unlike with inner joins, the watermark delay and event-time constraints are not
optional for outer joins.

• Consequently, the outer NULL results will be generated with a delay as the engine
has to wait for a while to ensure that there neither were nor would be any
matches.

- Arbitrary Stateful Computations

For example, say you want to track the statuses (e.g., signed in,
busy, idle) of users by tracking their activities (e.g., clicks) in real time. To build this
stream processing pipeline, you will have to track each user’s activity history as a state
with arbitrary data structure, and continuously apply arbitrarily complex changes on
the data structure based on the user’s actions. The operation mapGroupsWithState()
and its more flexible counterpart flatMapGroupsWithState() are designed for such
complex analytical use cases.

As of Spark 3.0, these two operations are only available in Scala and
Java.

- Modeling Arbitrary Stateful Operations with mapGroupsWithState()

Programmatically in Scala, you will have to define a function with the following 
signature (K, V, S, and U are data types, as explained shortly):

// In Scala
def arbitraryStateUpdateFunction(
key: K,
newDataForKey: Iterator[V],
previousStateForKey: GroupState[S]
): U

This function is provided to a streaming query using the operations groupByKey()
and mapGroupsWithState(), as follows:

// In Scala
val inputDataset: Dataset[V] = // input streaming Dataset
inputDataset
.groupByKey(keyFunction) // keyFunction() generates key from input
.mapGroupsWithState(arbitraryStateUpdateFunction)

When this streaming query is started, in each micro-batch Spark will call this
arbitraryStateUpdateFunction() for each unique key in the micro-batch’s data.

- key: K

K is the data type of the common keys defined in the state and the input

- newDataForKey: Iterator[V]

V is the data type of the input Dataset.

- previousStateForKey: GroupState[S]

S is the data type of the arbitrary state you are going to maintain, and Group
State[S] is a typed wrapper object that provides methods to access and manage
the state value.

- U

U is the data type of the output of the function.

In every micro-batch, for each active user, we will use the new
actions taken by the user and update the user’s “status.” Programmatically, we can
define the state update function with the following steps:
1. Define the data types. We need to define the exact types of K, V, S, and U. In this
case, we’ll use the following:

a. Input data (V) = case class UserAction(userId: String, action:
String)
b. Keys (K) = String (that is, the userId)
c. State (S) = case class UserStatus(userId: String, active: Bool
d. Output (U) = UserStatus

2. Define the function.

// In Scala
import org.apache.spark.sql.streaming._
def updateUserStatus(
userId: String,
newActions: Iterator[UserAction],
state: GroupState[UserStatus]): UserStatus = {

val userStatus = state.getOption.getOrElse {
new UserStatus(userId, false)
}
newActions.foreach { action =>
userStatus.updateWith(action)
}
state.update(userStatus)
return userStatus
}

3. Apply the function on the actions.

We will group the input actions Dataset
using groupByKey() and then apply the updateUserStatus function using
mapGroupsWithState():

// In Scala
val userActions: Dataset[UserAction] = ...
val latestStatuses = userActions
.groupByKey(userAction => userAction.userId)
.mapGroupsWithState(updateUserStatus _)

- Using Timeouts to Manage Inactive Groups

To encode time-based inactivity, mapGroupsWithState() supports timeouts that are
defined as follows:

• Each time the function is called on a key, a timeout can be set on the key based
on a duration or a threshold timestamp.
• If that key does not receive any data, such that the timeout condition is met, the
key is marked as “timed out.”

- Processing-time timeouts

Processing-time timeouts are based on the system time

// In Scala
def updateUserStatus(
userId: String,
newActions: Iterator[UserAction],
state: GroupState[UserStatus]): UserStatus = {
if (!state.hasTimedOut) { // Was not called due to timeout
val userStatus = state.getOption.getOrElse {
new UserStatus(userId, false)
}
newActions.foreach { action => userStatus.updateWith(action) }
state.update(userStatus)
state.setTimeoutDuration("1 hour") // Set timeout duration
return userStatus
} else {
val userStatus = state.get()
state.remove() // Remove state when timed out
return userStatus.asInactive() // Return inactive user's status
}
}
val latestStatuses = userActions
.groupByKey(userAction => userAction.userId)
.mapGroupsWithState(
GroupStateTimeout.ProcessingTimeTimeout)(
updateUserStatus _)

- Event-time timeouts

Instead of the system clock time, an event-time timeout is based on the event time in
the data (similar to time-based aggregations) and a watermark defined on the event
time.

• Define watermarks on the input Dataset (assume that the class UserAction has
an eventTimestamp field). Recall that the watermark threshold represents the
acceptable amount of time by which input data can be late and out of order.
• Update mapGroupsWithState() to use EventTimeTimeout.
• Update the function to set the threshold timestamp at which the timeout will
occur.

// In Scala
def updateUserStatus(
userId: String,
newActions: Iterator[UserAction],
state: GroupState[UserStatus]):UserStatus = {

if (!state.hasTimedOut) { // Was not called due to timeout
val userStatus = if (state.getOption.getOrElse {
new UserStatus()
}
newActions.foreach { action => userStatus.updateWith(action) }
state.update(userStatus)

// Set the timeout timestamp to the current watermark + 1 hour
state.setTimeoutTimestamp(state.getCurrentWatermarkMs, "1 hour")
return userStatus
} else {
val userStatus = state.get()
state.remove()
return userStatus.asInactive() }
}

val latestStatuses = userActions
.withWatermark("eventTimestamp", "10 minutes")
.groupByKey(userAction => userAction.userId)
.mapGroupsWithState(
GroupStateTimeout.EventTimeTimeout)(
updateUserStatus _)

For example, you can implement an approximately periodic task (say, every
hour) on the state by saving the last task execution timestamp in the state and using
that to set the processing-time timeout duration, as shown in this code snippet:

// In Scala
timeoutDurationMs = lastTaskTimstampMs + periodIntervalMs -
groupState.getCurrentProcessingTimeMs()

- Generalization with flatMapGroupsWithState()

There are two key limitations with mapGroupsWithState()

• Every time mapGroupsWithState() is called, you have to return one and only one
record.

• With mapGroupsWithState(), due to the lack of more information about the
opaque state update function, the engine assumes that generated records are
updated key/value data pairs.

flatMapGroupsWithState() overcomes these limitations, at the cost of slightly more
complex syntax. It has two differences from mapGroupsWithState(): 

• The return type is an iterator, instead of a single object. This allows the function
to return any number of records, or, if needed, no records at all.

• It takes another parameter, called the operator output mode (not to be confused
with the query output modes we discussed earlier in the chapter), that defines
whether the output records are new records that can be appended (Output
Mode.Append) or updated key/value records (OutputMode.Update).

Performance Tuning

Structured Streaming uses the Spark SQL engine and therefore can be tuned with the
same parameters as those discussed for Spark SQL

However, unlike batch jobs that may process gigabytes to terabytes of data, micro-batch jobs
usually process much smaller volumes of data. Hence, a Spark cluster running
streaming queries usually needs to be tuned slightly differently.

- Cluster resource provisioning

Since Spark clusters running streaming queries are going to run 24/7, it is important
to provision resources appropriately. Underprovisoning the resources can
cause the streaming queries to fall behind

- Number of partitions for shuffles

For Structured Streaming queries, the number of shuffle partitions usually needs
to be set much lower than for most batch queries

- Setting source rate limits for stability

- Multiple streaming queries in the same Spark application


*************************************************
9. BUILDING REALIBLE DATA LAKES WITH APACHE SPARK
*************************************************

The Importance of an Optimal Storage Solution

- Scalability and performance
- Transaction support -> ACID
- Support for diverse data formats
- Support for diverse workloads
- Openness

Databases

- A Brief Introduction to Databases

SQL workloads on databases can be broadly classified into two categories, as follows:

- Online transaction processing (OLTP) workloads
- Online analytical processing (OLAP) 

It is important to note that Apache Spark is a query engine that is primarily designed
for OLAP workloads, not OLTP workloads

Reading from and Writing to Databases Using Apache Spark

Thanks to the ever-growing ecosystem of connectors, Apache Spark can connect to a
wide variety of databases for reading and writing data. For databases that have JDBC
drivers (e.g., PostgreSQL, MySQL), you can use the built-in JDBC data source along
with the appropriate JDBC driver jars to access the data. For many other modern
databases (e.g., Azure Cosmos DB, Snowflake), there are dedicated connectors that
you can invoke using the appropriate format name.

Limitations of Databases

- Growth in data sizes
- Growth in the diversity of analytics


Databases have been shown to be rather inadequate at accommodating these new
trends

- Databases are extremely expensive to scale out
- Databases do not support non–SQL based analytics very well

Data Lakes

Data lake is a distributed storage solution that runs on
commodity hardware and easily scales out horizontally

- A Brief Introduction to Data Lakes

The data lake architecture, unlike that of databases, decouples the distributed storage
system from the distributed compute system. This allows each system to scale out as
needed by the workload. Furthermore, the data is saved as files with open formats,
such that any processing engine can read and write them using standard APIs

- Storage system
- File format
- Processing engine(s)

Reading from and Writing to Data Lakes using Apache Spark

- Support for diverse workloads
- Support for diverse file formats
- Support for diverse filesystems

Limitations of Data Lakes

- Atomicity and isolation
- Consistency

Lakehouses: The Next Step in the Evolution of Storage Solutions

The lakehouse is a new paradigm that combines the best elements of data lakes and
data warehouses for OLAP workloads

- Transaction support (ACID)
- Schema enforcement and governance
- Support for diverse data types in open formats
- Support for diverse workloads
- Support for upserts and deletes
- Data governance

Apache Hudi
Apache Iceberg

Delta Lake

Delta Lake is an open source project hosted by the Linux Foundation, built by the
original creators of Apache Spark. Similar to the others, it is an open data storage format
that provides transactional guarantees and enables schema enforcement and evolution.

• Streaming reading from and writing to tables using Structured Streaming sources
and sinks
• Update, delete, and merge (for upserts) operations, even in Java, Scala, and
Python APIs
• Schema evolution either by explicitly altering the table schema or by implicitly
merging a DataFrame’s schema to the table’s during the DataFrame’s write. (In
fact, the merge operation in Delta Lake supports advanced syntax for conditional
updates/inserts/deletes, updating all columns together, etc., as you’ll see later in
the chapter.)
• Time travel, which allows you to query a specific table snapshot by ID or by
timestamp
• Rollback to previous versions to correct errors
• Serializable isolation between multiple concurrent writers performing any SQL,
batch, or streaming operations

Building Lakehouses with Apache Spark and Delta Lake

Configuring Apache Spark with Delta Lake

- Set up an interactive shell

pyspark --packages io.delta:delta-core_2.12:0.7.0

- Set up a standalone Scala/Java project using Maven coordinates

If you want to build a project using Delta Lake binaries from the Maven Central
repository, you can add the following Maven coordinates to the project dependencies:

<dependency>
<groupId>io.delta</groupId>
<artifactId>delta-core_2.12</artifactId>
<version>0.7.0</version>
</dependency>

Loading Data into a Delta Lake Table

All you have to do is change all the DataFrame read and write
operations to use format("delta") instead of format("parquet").

// In Scala
// Configure source data path
val sourcePath = "/databricks-datasets/learning-spark-v2/loans/
loan-risks.snappy.parquet"
// Configure Delta Lake path
val deltaPath = "/tmp/loans_delta"
// Create the Delta table with the same loans data
spark
.read
.format("parquet")
.load(sourcePath)
.write
.format("delta")
.save(deltaPath)
// Create a view on the data called loans_delta
spark
.read
.format("delta")
.load(deltaPath)
.createOrReplaceTempView("loans_delta")

Loading Data Streams into a Delta Lake Table

You can append to the table as follows:

// In Scala
import org.apache.spark.sql.streaming._
val newLoanStreamDF = ... // Streaming DataFrame with new loans data
val checkpointDir = ... // Directory for streaming checkpoints
val streamingQuery = newLoanStreamDF.writeStream
.format("delta")
.option("checkpointLocation", checkpointDir)
.trigger(Trigger.ProcessingTime("10 seconds"))
.start(deltaPath)

# In Python
newLoanStreamDF = ... # Streaming DataFrame with new loans data
checkpointDir = ... # Directory for streaming checkpoints
streamingQuery = (newLoanStreamDF.writeStream
.format("delta")
.option("checkpointLocation", checkpointDir)
.trigger(processingTime = "10 seconds")
.start(deltaPath))

- It allows writes from both batch and streaming jobs into the same table
- It allows multiple streaming jobs to append data to the same table
- It provides ACID guarantees even under concurrent writes

Enforcing Schema on Write to Prevent Data Corruption

A common problem with managing data with Spark using common formats like
JSON, Parquet, and ORC is accidental data corruption caused by writing incorrectly
formatted data.

The Delta Lake format records the schema as table-level metadata. Hence, all writes
to a Delta Lake table can verify whether the data being written has a schema compatible
with that of the table.
If it is not compatible, Spark will throw an error before any data is written and committed to the table

Evolving Schemas to Accommodate Changing Data

This new column can be explicitly added by setting the option
"mergeSchema" to "true

// In Scala
loanUpdates.write.format("delta").mode("append")
.option("mergeSchema", "true")
.save(deltaPath)

Transforming Existing Data

Delta Lake supports the DML commands UPDATE, DELETE, and MERGE, which allow
you to build complex data pipelines. These commands can be invoked using Java,
Scala, Python, and SQL

- Updating data to fix errors

1. Copy all of the rows that are not affected into a new table.
2. Copy all of the rows that are affected into a DataFrame, then perform the data
modification.
3. Insert the previously noted DataFrame’s rows into the new table.
4. Remove the old table and rename the new table to the old table name.

However, with a Delta Lake table, users can run this operation
too, by using Delta Lake’s programmatic APIs as follows:

// In Scala
import io.delta.tables.DeltaTable
import org.apache.spark.sql.functions._
val deltaTable = DeltaTable.forPath(spark, deltaPath)
deltaTable.update(
col("addr_state") === "OR",
Map("addr_state" -> lit("WA")))


- Deleting user-related data

With Delta Lake, you can do the following:

// In Scala
val deltaTable = DeltaTable.forPath(spark, deltaPath)
deltaTable.delete("funded_amnt >= paid_amnt")

- Upserting change data to a table using merge()

You can upsert these changes into the table using the DeltaTable.merge() operation, which is based on
the MERGE SQL command:

// In Scala
deltaTable
.alias("t")
.merge(loanUpdates.alias("s"), "t.loan_id = s.loan_id")
.whenMatched.updateAll()
.whenNotMatched.insertAll()
.execute()

Deduplicating data while inserting using insert-only merge

- Delete actions
For example, MERGE ... WHEN MATCHED THEN DELETE.

- Clause conditions
For example, MERGE ... WHEN MATCHED AND <condition> THEN ....

-  Optional actions

All the MATCHED and NOT MATCHED clauses are optional.

- Star syntax

For example, UPDATE * and INSERT * to update/insert all the columns in the target
table with matching columns from the source data set.

Auditing Data Changes with Operation History

All of the changes to your Delta Lake table are recorded as commits in the table’s
transaction log.

You can query the table’s operation history as noted in the
following code snippet:

// In Scala/Python
deltaTable.history().show()


// In Scala
deltaTable
.history(3)
.select("version", "timestamp", "operation", "operationParameters")
.show(false)

Querying Previous Snapshots of a Table with Time Travel

You can query previous versioned snapshots of a table by using the DataFrameReader
options "versionAsOf" and "timestampAsOf

// In Scala
spark.read
.format("delta")
.option("timestampAsOf", "2020-01-01") // timestamp after table creation
.load(deltaPath)

This is useful in a variety of situations, such as:

• Reproducing machine learning experiments and reports by rerunning the job on
a specific table version
• Comparing the data changes between different versions for auditing
• Rolling back incorrect changes by reading a previous snapshot as a DataFrame
and overwriting the table with it


*******************************
10. MACHINE LEARNING WITH MLIB
*******************************

